{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "**COMMAND LINE PROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on machine ppjanka-razer\n"
     ]
    }
   ],
   "source": [
    "# check whether we're running in Jupyter or from a script file\n",
    "cmd_args = []\n",
    "import socket\n",
    "machine = socket.gethostname()\n",
    "print('Running on machine %s' % machine)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from copy import copy\n",
    "from shutil import rmtree # remove a directory recursively\n",
    "import tarfile\n",
    "import __main__\n",
    "in_script = hasattr(__main__, '__file__')\n",
    "if in_script:\n",
    "    print(\"Running paper1_dashboard in command line mode.\", flush=True)\n",
    "    if len(sys.argv) == 1: # print usage instructions\n",
    "        print('''Usage of paper1_dashboard:\n",
    "            > to process a dashboard for a single file:\n",
    "                python paper1_dashboard.py -dashboard <datapath with joined_vtk folder>\n",
    "            > to process a comparison dashboard:\n",
    "                python paper1_dashboard.py -comparison <datapath with joined_vtk folder> <datapath with joined_vtk folder>\n",
    "            > to perform an experiment regarding long-term Fsyn enhancement (see Pjanka et al. 2022)\n",
    "                python paper1_dashboard.py -expLongFsyn <datapath with joined_vtk folder for 1D case> <datapath with joined_vtk folder for 2D (corrugated) case>\n",
    "            > to process and plot magnetic field curvature diagnostics\n",
    "                python paper1_dashboard.py -curvature <datapath with joined_vtk folder>\n",
    "            > other options\n",
    "                -nproc - number of threads for parallel processing\n",
    "                -opt_tf <0/1> - turn off/on tensorflow parallelization\n",
    "                -opt_numba <0/1> - turn off/on numba optimization\n",
    "                -low_memory <0/1> - make adjustments to enable computing with low system memory available\n",
    "                -convert_vtk <0/1> - convert vtk files to .pkl\n",
    "                -tar_when_done <0/1> - if not already tarfiles, the datapath folders will be turned into *.tgz after the analysis is done\n",
    "                -force_recalc <0/1> - force recalculation from .vtk files even if suitable pre-calculated .pkl files are present''')\n",
    "        sys.exit()\n",
    "    cmd_args = sys.argv[1:]\n",
    "    \n",
    "# cast anything to bool, including strings\n",
    "def boolean (x):\n",
    "    return bool(int(x))\n",
    "    \n",
    "def get_arg (argname, n_read=1, default=None, val_type=str):\n",
    "    try:\n",
    "        idx = cmd_args.index('-'+argname)\n",
    "        if n_read == 1:\n",
    "            return val_type(cmd_args[idx+1])\n",
    "        else:\n",
    "            return [val_type[i](cmd_args[idx+1+i]) for i in range(n_read)]\n",
    "    except Exception as e:\n",
    "        if default != None:\n",
    "            return default\n",
    "        else:\n",
    "            print('Argument %s not specified, with no default value given. Aborting.' % argname, flush=True)\n",
    "            sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "**EXECUTION SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using nproc = 6\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Throughout this work, Bfield is defined such that U_B = B**2/2, unless stated otherwise\n",
    "\n",
    "# PROCESSING TYPE\n",
    "if '-dashboard' in cmd_args:\n",
    "    processing_type = 'dashboard'\n",
    "    datapath = get_arg('dashboard')\n",
    "    if '.tgz' not in datapath and datapath[-1] != '/':\n",
    "        datapath += '/'\n",
    "elif '-comparison' in cmd_args:\n",
    "    processing_type = 'comparison'\n",
    "    datapaths_comp = list(get_arg('comparison', n_read=2, val_type=[str,str]))\n",
    "    for i in range(2):\n",
    "        if '.tgz' not in datapaths_comp[i] and datapaths_comp[i][-1] != '/':\n",
    "            datapaths_comp[i] += '/'\n",
    "elif '-expLongFsyn' in cmd_args:\n",
    "    processing_type = 'expLongFsyn'\n",
    "    datapaths_comp = list(get_arg('expLongFsyn', n_read=2, val_type=[str,str]))\n",
    "    for i in range(2):\n",
    "        if '.tgz' not in datapaths_comp[i] and datapaths_comp[i][-1] != '/':\n",
    "            datapaths_comp[i] += '/'\n",
    "elif '-curvature' in cmd_args:\n",
    "    processing_type = 'curvature'\n",
    "    datapath = get_arg('curvature')\n",
    "    if '.tgz' not in datapath and datapath[-1] != '/':\n",
    "        datapath += '/'\n",
    "else: # default processing options\n",
    "    if False:\n",
    "        processing_type = 'curvature'\n",
    "        datapath = '/DATA/Dropbox/LOOTRPV/astro_projects/2020_IntSh2/athena4p2/bin_paper1/corrT2_press/prod1_corr_ampl/results_corr1ampl20'\n",
    "        if '.tgz' not in datapath and datapath[-1] != '/':\n",
    "            datapath += '/'\n",
    "    elif False:\n",
    "        processing_type = 'expLongFsyn'\n",
    "        datapaths_comp = [\n",
    "            '/DATA/Dropbox/LOOTRPV/astro_projects/2020_IntSh2/athena4p2/bin_paper1/corrT2_press/prod1_corr_ampl/test_corr0_ampl50',\n",
    "            '/DATA/Dropbox/LOOTRPV/astro_projects/2020_IntSh2/athena4p2/bin_paper1/corrT2_press/prod1_corr_ampl/test_corr1_ampl50'\n",
    "        ]\n",
    "        for i in range(2):\n",
    "            if datapaths_comp[i][-4:] != '.tgz' and datapaths_comp[i][-1] != '/':\n",
    "                datapaths_comp[i] += '/'\n",
    "    elif False:\n",
    "        processing_type = 'dashboard'\n",
    "        datapath = '/DATA/Dropbox/LOOTRPV/astro_projects/2020_IntSh2/athena4p2/bin_paper1/test_tf.tgz'\n",
    "        if '.tgz' not in datapath and datapath[-1] != '/':\n",
    "            datapath += '/'\n",
    "    else:\n",
    "        processing_type = 'comparison'\n",
    "        datapaths_comp = [\n",
    "            '/DATA/Dropbox/LOOTRPV/astro_projects/2020_IntSh2/athena4p2/bin_paper1/corrT2_press/prod1_corr_ampl/results_corr0ampl50',\n",
    "            '/DATA/Dropbox/LOOTRPV/astro_projects/2020_IntSh2/athena4p2/bin_paper1/corrT2_press/prod1_corr_ampl/results_corr1ampl50'\n",
    "        ]\n",
    "        for i in range(2):\n",
    "            if datapaths_comp[i][-4:] != '.tgz' and datapaths_comp[i][-1] != '/':\n",
    "                datapaths_comp[i] += '/'\n",
    "\n",
    "# MAIN EXECUTION PARAMETERS\n",
    "unit_check = False # turns off optimization but allows to use astropy to check the units\n",
    "tar_when_done = get_arg('tar_when_done', default=False, val_type=boolean) # if not already a tarfile, datapath will be turned into .tgz at the end of analysis\n",
    "opt_tf = get_arg('opt_tf', default=False, val_type=boolean) # use tensorflow instead of numpy for GPU acceleration, x5 speedup\n",
    "opt_numba = get_arg('opt_numba', default=True, val_type=boolean) # use numba to pre-compile some of the functions, about 3x speedup (turned off if opt_tf is on and GPU available)\n",
    "low_memory = get_arg('low_memory', default=False, val_type=boolean) # make adjustments to enable computing with low system memory available\n",
    "convert_vtk = get_arg('convert_vtk', default=True, val_type=boolean) # saves vtk and processing data as pkl -- needed to use tensorflow, but also prevents from recalculating the same data\n",
    "force_recalc = get_arg('force_recalc', default=False, val_type=boolean) # force recalculation of augmented data, even if present in the pkl files\n",
    "\n",
    "import multiprocessing\n",
    "cpu_avail = multiprocessing.cpu_count()\n",
    "nproc = get_arg('nproc', default=int(0.5*cpu_avail), val_type=int)\n",
    "nproc_history = get_arg('nproc_history', default=1, val_type=int)\n",
    "if nproc < 0:\n",
    "    nproc = cpu_avail\n",
    "if nproc_history < 0:\n",
    "    nproc_history = cpu_avail\n",
    "print(f'Using nproc = {nproc}, nproc_history = {nproc_history}', flush=True)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# optimization logic\n",
    "opt_tf = (not unit_check and opt_tf)\n",
    "if opt_tf:\n",
    "    import tensorflow as tf\n",
    "    # do not use tensorflow if no or insufficient GPU detected (tf requires compute capability > 3.5), use numba instead\n",
    "    gpu_list = tf.config.list_physical_devices('GPU')\n",
    "    # if running on tegner\n",
    "    if machine[0] == 't' and machine[-11:] == '.pdc.kth.se':\n",
    "        # only the double-GPU nodes on tegner have sufficient compute capability to use tensorflow\n",
    "        opt_tf = (len(gpu_list) > 1)\n",
    "    else:\n",
    "        # otherwise use tf if any gpu available\n",
    "        opt_tf = (len(gpu_list) > 0)\n",
    "\n",
    "opt_numba = (not unit_check and not opt_tf and opt_numba)\n",
    "opt_fastmath = (opt_numba and True) # 20% speedup, but less precision\n",
    "\n",
    "# general imports\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from collections.abc import Iterable\n",
    "from numba import jit, prange\n",
    "\n",
    "# vtk file loading if available\n",
    "try:\n",
    "    import vtk as _\n",
    "    from read_vtk import vtk\n",
    "except ImportError:\n",
    "    def vtk(x, out_dt):\n",
    "        print('VTK module not available. Please convert to pkl files and use those instead.', flush=True)\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorflow optimization wrappers\n",
    "if opt_tf:\n",
    "    print(\"Tensorflow %s\" % tf.__version__)\n",
    "    print(gpu_list)\n",
    "    def tf_convert (*arg):\n",
    "        '''Convert all arguments to tf.const for GPU processing'''\n",
    "        return (tf.convert_to_tensor(x, dtype='float64') for x in arg)\n",
    "    def tf_deconvert (x):\n",
    "        '''Use tensor.to_numpy() to perform tf operations'''\n",
    "        return x.numpy()\n",
    "    where = tf.where\n",
    "    exp = tf.math.exp\n",
    "    linspace = tf.linspace\n",
    "    meshgrid = tf.meshgrid\n",
    "    npsum = tf.math.reduce_sum\n",
    "    log = tf.math.log\n",
    "    reshape = tf.reshape\n",
    "    sqrt = tf.math.sqrt\n",
    "    npmax = tf.reduce_max\n",
    "    def logspace (xmin,xmax,n):\n",
    "        return 10.**tf.linspace(xmin,xmax,n)\n",
    "    def log10 (x):\n",
    "        ten, = tf_convert(10.)\n",
    "        return tf.math.log(x) / tf.math.log(ten)\n",
    "    def nansum (x, axis):\n",
    "        return tf.math.reduce_sum(tf.where(tf.math.is_nan(x),tf.zeros_like(x),x), axis=axis)\n",
    "    cos = tf.math.cos\n",
    "    sin = tf.math.sin\n",
    "    arctan = tf.math.atan\n",
    "    arccos = tf.math.acos\n",
    "    modulo = tf.experimental.numpy.mod\n",
    "    repeat = tf.repeat\n",
    "    top_k = tf.math.top_k\n",
    "    expand_dims = tf.expand_dims\n",
    "    npsort = tf.sort\n",
    "    nptake = tf.experimental.numpy.take\n",
    "    nptake_along_axis = tf.experimental.numpy.take_along_axis\n",
    "    transpose = tf.transpose\n",
    "    npstack = tf.stack\n",
    "    swapaxes = tf.experimental.numpy.swapaxes\n",
    "    moveaxis = tf.experimental.numpy.moveaxis\n",
    "    npmean = tf.reduce_mean\n",
    "    npmax = tf.max\n",
    "else:\n",
    "    @jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "    def tf_convert (*arg):\n",
    "        return arg\n",
    "    @jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "    def tf_deconvert (arg):\n",
    "        return arg\n",
    "    where = np.where\n",
    "    exp = np.exp\n",
    "    linspace = np.linspace\n",
    "    meshgrid = np.meshgrid\n",
    "    npsum = np.sum\n",
    "    log = np.log\n",
    "    reshape = np.reshape\n",
    "    sqrt = np.sqrt\n",
    "    npmax = np.nanmax\n",
    "    logspace = np.logspace\n",
    "    log10 = np.log10\n",
    "    nansum = np.nansum\n",
    "    cos = np.cos\n",
    "    sin = np.sin\n",
    "    arctan = np.arctan\n",
    "    arccos = np.arccos\n",
    "    modulo = np.mod\n",
    "    repeat = np.repeat\n",
    "    #@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "    def top_k (x, k):\n",
    "        return np.argpartition(\n",
    "            x, k, axis=-1\n",
    "        )[...,:k]\n",
    "    expand_dims = np.expand_dims\n",
    "    npsort = np.sort\n",
    "    nptake = np.take\n",
    "    nptake_along_axis = np.take_along_axis\n",
    "    transpose = np.transpose\n",
    "    npstack = np.stack\n",
    "    swapaxes = np.swapaxes\n",
    "    moveaxis = np.moveaxis\n",
    "    npmean = np.mean\n",
    "    npabs = np.abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "from astropy.constants import e, c, m_e\n",
    "import astropy.units as u\n",
    "from scipy.special import gamma\n",
    "e = e.esu.value * u.cm**1.5 * u.g**0.5 / u.s\n",
    "\n",
    "if unit_check:\n",
    "    #units\n",
    "    cm = u.cm\n",
    "    gram = u.g\n",
    "    sec = u.s\n",
    "    Hz = u.Hz\n",
    "    kpc = u.kpc\n",
    "    Msun = u.Msun\n",
    "    erg = u.erg\n",
    "    def get_cgs (x): return x.cgs\n",
    "    def get_cgs_value (x): return x.cgs.value\n",
    "else:\n",
    "    #constants\n",
    "    e = e.cgs.value\n",
    "    c = c.cgs.value\n",
    "    m_e = m_e.cgs.value\n",
    "    #units\n",
    "    cm = 1.0\n",
    "    gram = 1.0\n",
    "    sec = 1.0\n",
    "    Hz = 1.0\n",
    "    kpc = (1.*u.kpc).cgs.value\n",
    "    Msun = (1.*u.Msun).cgs.value\n",
    "    erg = (1.*u.erg).cgs.value\n",
    "    @jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "    def get_cgs (x): return x\n",
    "    @jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "    def get_cgs_value (x): return x\n",
    "    \n",
    "# simulation units\n",
    "# anchor points\n",
    "simu_rho = get_cgs(10**(-15) * gram/cm**3)\n",
    "simu_t = get_cgs(1.*sec)\n",
    "simu_len = get_cgs(c*simu_t)\n",
    "# derivative units\n",
    "simu_mass = get_cgs(simu_rho * simu_len**3)\n",
    "simu_en = simu_mass * get_cgs(c)**2 # erg\n",
    "simu_press = simu_en / simu_len**3 # erg / cm^3\n",
    "simu_B = np.sqrt(2.*simu_press) # sqrt( erg / cm^3 )\n",
    "simu_B_8piCorr = np.sqrt(8.*np.pi*simu_press) # sqrt( erg / cm^3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "--------------------\n",
    "**PHYSICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROBLEM PARAMETERS\n",
    "out_dt_vtk = 0.1\n",
    "adiab_idx = 1.33333333333\n",
    "\n",
    "# general parameters, see Malzac (2014,2018)\n",
    "gmin = 1.0e1\n",
    "gmax = 1.0e6\n",
    "p = 2.5\n",
    "xi_e = 1.0 # electron equipartition parameter, U_e = xi_e* U_B\n",
    "\n",
    "# choice of emitting region size and frequency for plotting\n",
    "#R_choice = 6.0e-18 # ensures tau~1 at nu~10^14 (in the IR), it is low due to our high Bfield\n",
    "R_choice = 6.0e8 # IR core, see Shidatsu et al. (2011)\n",
    "nu_choice = 1.4e14 # Hz # see Malzac et al. (2018)\n",
    "\n",
    "# observer-frame frequency range for flux integration\n",
    "# IR-opt range: 300GHz - 3PHz\n",
    "nu_int_min = 300.0e9\n",
    "nu_int_max = 3.0e15\n",
    "\n",
    "# system properties (see overleaf)\n",
    "gamma_jet = 2.0\n",
    "beta_jet = np.sqrt(1.0 - 1.0/gamma_jet**2) # old bug: 0.75\n",
    "incl = 30.0 * np.pi / 180.\n",
    "theta_j = 2.3 * np.pi / 180.\n",
    "dist = 8. * kpc\n",
    "mbh = 10. * Msun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATHS\n",
    "\n",
    "# vector operations\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def sqr_vec_l2 (x,y,z):\n",
    "    x,y,z = tf_convert(x,y,z)\n",
    "    return x**2 + y**2 + z**2\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def norm_vec_l2 (x,y,z):\n",
    "    x,y,z = tf_convert(x,y,z)\n",
    "    return sqrt(x**2 + y**2 + z**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIAL RELATIVITY\n",
    "\n",
    "# NOTE: in Athena 4.2 vel1, vel2, vel3 are 3-velocities. Instead Athena++ uses 4-velocities in the code (so they would need to be translated).\n",
    "\n",
    "# v <-> gamma convenience functions\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def v2gamma (v):\n",
    "    v, = tf_convert(v)\n",
    "    return 1.0/sqrt(1.0-v**2)\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def vsqr2gamma (vsqr):\n",
    "    vsqr, = tf_convert(vsqr)\n",
    "    return 1.0/sqrt(1.0-vsqr)\n",
    "\n",
    "# velocity composition\n",
    "@jit(nopython=opt_numba, parallel=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def combined_beta_vec (beta_vec):\n",
    "    '''Combines the fluid beta with the bulk jet motion (general case).'''\n",
    "    beta_obs = np.zeros(beta_vec.shape)\n",
    "    beta_obs[...,0] = (beta_vec[...,0] + beta_jet) / (1.0 + beta_vec[...,0]*beta_jet)\n",
    "    for i in (1, 2):\n",
    "        beta_obs[...,i] = sqrt(1.0 - beta_jet**2) * beta_vec[...,i] / (1.0 + beta_vec[...,0]*beta_jet)\n",
    "    return beta_obs\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def combined_beta (beta_vec):\n",
    "    '''Combines the fluid beta with the bulk jet motion.'''\n",
    "    beta_vec = combined_beta_vec(beta_vec)\n",
    "    beta = sqrt(npsum(beta_vec**2, axis=-1))\n",
    "    return beta_vec, beta\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def combined_gamma (beta_vec):\n",
    "    '''Combines the fluid beta with the bulk jet motion.'''\n",
    "    beta_vec, beta = combined_beta(beta_vec)\n",
    "    gamma = v2gamma(beta)\n",
    "    return beta_vec, beta, gamma\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def combined_doppler_factor (beta_vec, n_vec):\n",
    "    '''Combines the fluid beta with the bulk jet motion to get the total doppler factor.'''\n",
    "    beta_vec, beta, gamma = combined_gamma(beta_vec)\n",
    "    df = 1.0 / (gamma*(1.0-npsum(beta_vec*n_vec, axis=-1)))\n",
    "    return beta_vec, beta, gamma, df\n",
    "\n",
    "# Bcc in the fluid frame\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def BccFl0 (gamma, v1,v2,v3, b1,b2,b3):\n",
    "    gamma, v1,v2,v3, b1,b2,b3 = tf_convert(gamma, v1,v2,v3, b1,b2,b3)\n",
    "    return gamma * (v1*b1 + v2*b2 + v3*b3)\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def BccFli (gamma, v,b, bFl0):\n",
    "    gamma, v,b, bFl0 = tf_convert(gamma, v,b, bFl0)\n",
    "    return b / gamma + bFl0 * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SYNCHROTRON EMISSION\n",
    "\n",
    "# See Malzac (2014), appendix A\n",
    "\n",
    "if p != 2.0:\n",
    "    gamma_e_bar = ((1-p)/(2-p)) * (gmax**(2-p) - gmin**(2-p)) / (gmax**(1-p) - gmin**(1-p))\n",
    "else:\n",
    "    gamma_e_bar = (1-p) * np.log(gmax/gmin) / (gmax**(1-p) - gmin**(1-p))\n",
    "\n",
    "i_gamma = ( gamma_e_bar * (1/(1-p)) * (gmax**(1-p) - gmin**(1-p)) )**(-1)\n",
    "\n",
    "K_j = ( (np.sqrt(3.) * e**3 * i_gamma) / (16*np.pi**2 * m_e**2 * c**4 * (p+1)) ) \\\n",
    "    * gamma((3*p+19)/12.) * gamma((3*p-1)/12.) * (m_e*c/(3*e))**(-(p-1)/2.)\n",
    "    \n",
    "K_a = ( (np.sqrt(3.) * e**3 * i_gamma) / (64*np.pi**2 * m_e**3 * c**4) ) \\\n",
    "    * (3.*e / (2.*np.pi*m_e*c))**(0.5*p) \\\n",
    "    * gamma((3.*p+2)/12.) * gamma((3.*p+22)/12.)\n",
    "    \n",
    "# j_nu, nu in fluid frame\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def j_nu (nu_fl, B):\n",
    "    '''Warning: B is expected in simulation units, and is converted to physical here.\n",
    "    Unit: g / (cm s**2) = erg / cm**3'''\n",
    "    nu_fl, B = tf_convert(nu_fl, B*simu_B_8piCorr)\n",
    "    return get_cgs(K_j * xi_e * (B*(erg/cm**3)**0.5)**((p+5)/2.) * (nu_fl*Hz)**(-(p-1)/2.))\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def alpha_nu (nu_fl, B):\n",
    "    '''Warning: B is expected in simulation units, and is converted to physical here.\n",
    "    Unit: 1/cm'''\n",
    "    nu_fl, B = tf_convert(nu_fl, B*simu_B_8piCorr)\n",
    "    return get_cgs(K_a * xi_e * (B*(erg/cm**3)**0.5)**(0.5*p+3) * (nu_fl*Hz)**(-(p+4)/2.))\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def j_over_alpha_nu (nu_fl, B):\n",
    "    '''Warning: B is expected in simulation units, and is converted to physical here.\n",
    "    Unit: erg / (cm**2)'''\n",
    "    nu_fl, B = tf_convert(nu_fl, B*simu_B_8piCorr)\n",
    "    return get_cgs((K_j/K_a) * (B*(erg/cm**3)**0.5)**(-0.5) * (nu_fl*Hz)**(2.5))\n",
    "@jit(nopython=opt_numba, parallel=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def intensity (nu_fl, B, R):\n",
    "    '''Warning: B is expected in simulation units.\n",
    "    Unit: g/s**2 = erg / cm**2'''\n",
    "    nu_fl, B, R = tf_convert(nu_fl, B, R)\n",
    "    # for small alphae, we need to use a Taylor expansion\n",
    "    alpha = alpha_nu(nu_fl,B)\n",
    "    #return j_nu(nu_fl, B) * R*cm\n",
    "    return where(\n",
    "        alpha*R*cm < 1.0e-3,\n",
    "        j_nu(nu_fl, B) * R*cm,\n",
    "        j_over_alpha_nu(nu_fl, B) * (1.0-exp(-alpha*R*cm))\n",
    "    )\n",
    "    \n",
    "# observables\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def nu2nu_fl (nu, doppler_factor):\n",
    "    return nu / doppler_factor\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def flux_nu_per_dS (nu, B, gamma, doppler_factor, R, filling_factor=1.0):\n",
    "    '''Observer-frame synchrotron flux from the given simulation unit surface.\n",
    "    Unit: erg / (s cm**2 Hz) / cm**2'''\n",
    "    nu, B, gamma, doppler_factor, R, filling_factor = tf_convert(nu, B, gamma, doppler_factor, R, filling_factor)\n",
    "    return (doppler_factor**2 * gamma / (2.*dist**2)) \\\n",
    "             * filling_factor* intensity(nu2nu_fl(nu,doppler_factor),B,R)\n",
    "             #* (dS*simu_len**2) # perp. surface element, instead of (beta c dt R)\n",
    "    \n",
    "if not low_memory:\n",
    "    # total flux within the IR-opt range: 300GHz - 3PHz\n",
    "    @jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "    def _flux_total_integrate (\n",
    "        nu_grid,B_grid, gamma_grid,df_grid,R,\n",
    "        dlognu_grid, filling_factor=1.0\n",
    "    ):\n",
    "        nu_grid,B_grid,gamma_grid,df_grid,R,dlognu_grid, filling_factor = tf_convert(nu_grid,B_grid,gamma_grid,df_grid,R,dlognu_grid, filling_factor)\n",
    "        integrand = nu_grid*Hz * get_cgs(flux_nu_per_dS(nu_grid,B_grid,gamma_grid,df_grid,R,filling_factor))\n",
    "        return npsum(integrand*dlognu_grid, axis=-1)\n",
    "    # Do NOT use numba for flux_total. It does not understand np.meshgrid, and alternative implementations cause the code to be extremely slow.\n",
    "    def flux_total_per_dS (B,gamma,df,R, nu_min=nu_int_min, nu_max=nu_int_max, resolution=128, filling_factor=1.0):\n",
    "        '''Total observer-frame flux from dS within the IR-opt range: 300GHz - 3PHz.\n",
    "        Unit: erg/(cm**4*sec)'''\n",
    "        B,gamma,df,R, nu_min,nu_max = tf_convert(B,gamma,df,R, nu_min,nu_max)\n",
    "        Bshape = B.shape\n",
    "        # log-integrate the flux\n",
    "        lognu = linspace(log(nu_min), log(nu_max), resolution)\n",
    "        dlognu = lognu[1:] - lognu[:-1]\n",
    "        lognu = 0.5 * (lognu[1:] + lognu[:-1])\n",
    "        nu = exp(lognu)\n",
    "        B_grid, nu_grid = meshgrid(B, nu, indexing='ij')\n",
    "        gamma_grid, dlognu_grid = meshgrid(gamma, dlognu, indexing='ij')\n",
    "        df_grid, dlognu_grid = meshgrid(df, dlognu, indexing='ij')\n",
    "        return get_cgs(reshape(_flux_total_integrate(nu_grid,B_grid,gamma_grid,df_grid,R, dlognu_grid, filling_factor), Bshape))\n",
    "    \n",
    "else: # low-memory\n",
    "    @jit(nopython=opt_numba, parallel=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "    def flux_total_per_dS (B,gamma,df,R, nu_min=nu_int_min, nu_max=nu_int_max, resolution=128, filling_factor=1.0):\n",
    "        '''Total observer-frame flux from dS within the IR-opt range: 300GHz - 3PHz.\n",
    "        Unit: erg/(cm**4*sec)'''\n",
    "        B,gamma,df,R, nu_min,nu_max = tf_convert(B,gamma,df,R, nu_min,nu_max)\n",
    "        Bshape = B.shape\n",
    "        # log-integrate the flux\n",
    "        lognu = linspace(log(nu_min), log(nu_max), resolution)\n",
    "        dlognu = lognu[1:] - lognu[:-1]\n",
    "        lognu = 0.5 * (lognu[1:] + lognu[:-1])\n",
    "        nu = exp(lognu)\n",
    "        result = np.zeros(Bshape)\n",
    "        for idx in range(len(nu)):\n",
    "            nu_here = nu[idx]\n",
    "            dlognu_here = dlognu[idx]\n",
    "            result += nu_here*Hz * get_cgs(flux_nu_per_dS(nu_here, B, gamma, df, R, filling_factor)) * dlognu_here\n",
    "        return get_cgs(result)\n",
    "    \n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def syn_emission_rate_per_dS (flux_total_per_dS, doppler_factor, filling_factor=1.0):\n",
    "    '''Total synchrotron emision rate, averaged over the box surface, observer frame.\n",
    "    Unit: erg / (cm^2 s)'''\n",
    "    flux_total_per_dS, doppler_factor = tf_convert(flux_total_per_dS, doppler_factor)\n",
    "    return (8*np.pi*dist**2 / filling_factor) * npmean(flux_total_per_dS / doppler_factor**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if unit_check:\n",
    "    %timeit j_nu(12., 12.)\n",
    "    print(j_nu(12., 12.))\n",
    "    print(alpha_nu(12., 12.))\n",
    "    print(intensity(12., 12.,12.))\n",
    "    print(flux_total_per_dS(np.array([12.,]),np.array([12.,]),12.,0.5))\n",
    "    if unit_check:\n",
    "        print(\"Check that dimensionless: \", get_cgs(j_over_alpha_nu(12., 12.) / (erg / (cm**2))),\n",
    "          get_cgs(flux_total(np.array([12.,]),12.,0.5)) / get_cgs(erg/(cm**2*sec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLASMA PARAMETERS\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def plasma_beta (press, Bflsqr):\n",
    "    press, Bflsqr = tf_convert(press, Bflsqr)\n",
    "    return 2.*press / Bflsqr\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def magnetization (Bflsqr, rho):\n",
    "    Bflsqr, rho = tf_convert(Bflsqr, rho)\n",
    "    return 0.5 * Bflsqr / rho\n",
    "\n",
    "# internal energy in the fluid frame, see Beckwith & Stone (2011)\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def enthalpy (adiab_idx, press, rho):\n",
    "    '''Fluid-frame gas enthalpy per unit mass.'''\n",
    "    adiab_idx, press, rho = tf_convert(adiab_idx, press, rho)\n",
    "    return 1.0 + ((adiab_idx)/(adiab_idx-1.0)) * press / rho\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def internal_energy (rho, enthalpy, gamma, press, Bflsqr):\n",
    "    '''Fluid-frame total internal energy per unit volume.\n",
    "     - see Beckwith & Stone (2011), https://github.com/PrincetonUniversity/athena/wiki/Special-Relativity'''\n",
    "    rho, enthalpy, gamma, press, Bflsqr = tf_convert(rho, enthalpy, gamma, press, Bflsqr)\n",
    "    return rho * enthalpy * gamma**2 - press + 0.5 * Bflsqr # warning!: includes rest mass\n",
    "\n",
    "# total available kinetic energy in the observer frame\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def ekin_observer (gamma, rho):\n",
    "    '''Kinetic energy density in observer frame.\n",
    "     - cf. Beckwith & Stone (2011), eq. (20)'''\n",
    "    gamma, rho = tf_convert(gamma, rho)\n",
    "    return gamma**2 * rho\n",
    "\n",
    "# total available energy in the observer frame\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def etot_observer (beta_vec, beta_sqr, gamma, rho, enthalpy, press, Bfluid_vec, Bfluid_sqr):\n",
    "    '''Total energy per dS in observer frame.\n",
    "     - cf. Beckwith & Stone (2011), eq. (20)'''\n",
    "    beta_vec, beta_sqr, gamma, rho, enthalpy, press, Bfluid_vec, Bfluid_sqr = tf_convert(beta_vec, beta_sqr, gamma, rho, enthalpy, press, Bfluid_vec, Bfluid_sqr)\n",
    "    return rho * enthalpy * gamma**2 - press + 0.5 * beta_sqr * Bfluid_sqr - 0.5 * npsum(beta_vec * Bfluid_vec, axis=-1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stokes parameters\n",
    "# - ref. Lyutikov et al. (2005)\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def sin_local_incl (beta_vec, beta, n_vec):\n",
    "    return sqrt(1-npsum((beta_vec/beta.repeat(3).reshape(beta_vec.shape)) * n_vec, axis=-1)**2)\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_kappa (nu):\n",
    "    nu = tf_convert(nu)[0]\n",
    "    return 0.25*sqrt(3) * gamma((3*p-1)/12) * gamma((3*p+7)/12) * (e**3 / (me*c**2)) * (3*e / (2*np.pi*me**3*c**5))**(0.5*(p-1)) * nu**(-0.5*(p-1))\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_I (sin_local_incl, doppler_factor, Bfluid, sin_xiprime):\n",
    "    '''Stokes I parameter, before integrating over dS\n",
    "     - see Lyutikov et al. (2005), eq. (2).\n",
    "     - z=0 assumed,\n",
    "     - as we only use ratios here, part of normalization identical between I, Q, and U is omitted.'''\n",
    "    sin_local_incl, doppler_factor, Bfluid, sin_xiprime = tf_convert(sin_local_incl, doppler_factor, Bfluid, sin_xiprime)\n",
    "    return ((p+7/3)/(p+1)) * (1 / sin_local_incl) * doppler_factor**(2+0.5*(p-1)) * npabs(Bfluid*sin_xiprime)**(0.5*(p+1))\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_Q (sin_local_incl, doppler_factor, Bfluid, sin_xiprime, cos_xitilde):\n",
    "    '''Stokes Q parameter, before integrating over dS\n",
    "     - see Lyutikov et al. (2005), eq. (2).\n",
    "     - z=0 assumed,\n",
    "     - as we only use ratios here, part of normalization identical between I, Q, and U is omitted.'''\n",
    "    sin_local_incl, doppler_factor, Bfluid, sin_xiprime, cos_xitilde = tf_convert(sin_local_incl, doppler_factor, Bfluid, sin_xiprime, cos_xitilde)\n",
    "    return (1 / sin_local_incl) * doppler_factor**(2+0.5*(p-1)) * npabs(Bfluid*sin_xiprime)**(0.5*(p+1)) * (2*cos_xitilde**2 - 1.0)\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_U (sin_local_incl, doppler_factor, Bfluid, sin_xiprime, sin_xitilde, cos_xitilde):\n",
    "    '''Stokes U parameter, before integrating over dS\n",
    "     - see Lyutikov et al. (2005), eq. (2).\n",
    "     - z=0 assumed,\n",
    "     - as we only use ratios here, part of normalization identical between I, Q, and U is omitted.'''\n",
    "    sin_local_incl, doppler_factor, Bfluid, sin_xiprime, sin_xitilde, cos_xitilde = tf_convert(sin_local_incl, doppler_factor, Bfluid, sin_xiprime, sin_xitilde, cos_xitilde)\n",
    "    return (1 / sin_local_incl) * doppler_factor**(2+0.5*(p-1)) * npabs(Bfluid*sin_xiprime)**(0.5*(p+1)) * 2.*sin_xitilde*cos_xitilde\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_qprime (Bfluid_vec, beta_vec, gamma, n_vec):\n",
    "    '''Lyutikov et al. (2005), eq. (6).'''\n",
    "    gamma_vec = gamma.repeat(3).reshape(beta_vec.shape)\n",
    "    return Bfluid_vec + np.cross(n_vec, np.cross(beta_vec, Bfluid_vec)) - (gamma_vec/(1+gamma_vec)) * np.cross(Bfluid_vec, beta_vec) * beta_vec\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_e (n_vec, qprime_vec):\n",
    "    '''Lyutikov et al. (2005), eq. (6).'''\n",
    "    ncrossq = np.cross(n_vec, qprime_vec)\n",
    "    return ncrossq / sqrt(\n",
    "        npsum(qprime_vec**2, axis=-1) - npsum(ncrossq**2, axis=-1)\n",
    "    ).repeat(3).reshape(ncrossq.shape)\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_cos_xitilde (e_vec, n_vec, l_vec):\n",
    "    '''Lyutikov et al. (2005), eq. (5).'''\n",
    "    return np.sqrt(np.sum(\n",
    "        np.cross(e_vec, np.cross(n_vec, l_vec))**2, axis=-1\n",
    "    ))\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_sin_xitilde (e_vec, l_vec):\n",
    "    '''Lyutikov et al. (2005), eq. (5).'''\n",
    "    return np.sqrt(np.sum(\n",
    "        np.cross(e_vec, l_vec)**2, axis=-1\n",
    "    ))\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_nprime (n_vec, gamma, beta_vec):\n",
    "    '''Lyutikov et al. (2005), eq. (6).'''\n",
    "    gamma_vec = gamma.repeat(3).reshape(beta_vec.shape)\n",
    "    return (n_vec + gamma_vec * beta_vec * ((gamma_vec/(gamma_vec+1))*np.cross(n_vec, beta_vec)-1)) / (gamma_vec * (1 - np.cross(n_vec, beta_vec)))\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def stokes_sin_xiprime (nprime_vec, Bfluid_vec, Bfluid):\n",
    "    return sqrt(1 - npsum(nprime_vec*Bfluid_vec / Bfluid.repeat(3).reshape(Bfluid_vec.shape), axis=-1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGNETIC FIELD CURVATURE\n",
    "\n",
    "# TODO: could probably optimize it better, also: NOT tested for tensorflow yet\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def _vector_orientation (vectors):\n",
    "    '''Returns orientation phi (Arc tan(y/x)) of the vector field.'''\n",
    "    result = arctan(vectors[...,1] / vectors[...,0])\n",
    "    result = where(\n",
    "        vectors[...,0] > 0,\n",
    "        result,\n",
    "        result - np.pi\n",
    "    )\n",
    "    result = modulo(result, 2.*np.pi)\n",
    "    return result\n",
    "\n",
    "# we look at Bfield in counterclockwise manner, shifting by (dx,dy)\n",
    "# note the margin to ensure periodicity\n",
    "_phi_angles = tuple(np.arange(-0.75*np.pi, 2.75*np.pi, 0.25*np.pi))\n",
    "_shifts = (\n",
    "    (-1,-1), (0,-1), (1,-1),\n",
    "    (1,0), (1,1), (0,1), (-1,1), (-1,0), (-1,-1), (0,-1), (1,-1),\n",
    "    (1,0), (1,1), (0,1)\n",
    ")\n",
    "#@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def _bfield_angles (bfield, bfield_phi):\n",
    "    '''For each cell, we limit ourselves to the three neighbours closest to the direction of Bfield in that cell. Then, we report the angles between center cell's Bfield and Bfield at these neighbours.'''\n",
    "    # calculate which neighbouring cells are closest to where Bfield is pointing\n",
    "    direction_distances = repeat(expand_dims(bfield_phi,-1),len(_phi_angles), axis=-1) - _phi_angles\n",
    "    direction_idxs = top_k(\n",
    "        np.reshape(np.abs(direction_distances), (-1,len(_phi_angles))),\n",
    "        k=3\n",
    "    )\n",
    "    direction_idxs = reshape(direction_idxs, (*(direction_distances.shape[:-1]),3))\n",
    "    npsort(direction_idxs, axis=-1)\n",
    "    direction_distances = nptake_along_axis(\n",
    "        direction_distances,\n",
    "        direction_idxs,\n",
    "        axis=-1\n",
    "    )\n",
    "    # calculate angles between Bfield vectors\n",
    "    shift_vectors = nptake(\n",
    "        np.array(_shifts),\n",
    "        direction_idxs.flatten(),\n",
    "        axis=0\n",
    "    )\n",
    "    shift_vectors = reshape(\n",
    "        shift_vectors,\n",
    "        (*(bfield_phi.shape),3,2)\n",
    "    )\n",
    "    shift_vectors = transpose(shift_vectors, (-2,-1,0,1))\n",
    "    indices = np.meshgrid(range(bfield.shape[0]), range(bfield.shape[1]), indexing='ij')\n",
    "    indices = np.stack(indices)\n",
    "    indices = np.stack(3*[indices,])[...,1:-1,1:-1]\n",
    "    rolled_indices = indices + shift_vectors\n",
    "    bfield_rolled = np.array([\n",
    "        bfield[i,j] for i,j in rolled_indices\n",
    "    ])\n",
    "    bfield_here = npstack(3*[bfield[...,1:-1,1:-1,:],])\n",
    "    bfield_angles = arccos(\n",
    "        npsum(\n",
    "            bfield_here * bfield_rolled, axis=-1\n",
    "        ) / sqrt(npsum(bfield_here**2, -1) * npsum(bfield_rolled**2, -1))\n",
    "    )\n",
    "    bfield_angles = transpose(swapaxes(bfield_angles, 0,-1), (1,0,2))\n",
    "    \n",
    "    # clean up and return\n",
    "    del direction_idxs, shift_vectors, indices, rolled_indices, bfield_rolled, bfield_here\n",
    "    return direction_distances, bfield_angles\n",
    "\n",
    "@jit(nopython=opt_numba, fastmath=opt_fastmath, forceobj=(not opt_numba))\n",
    "def _extract_bfield_curvature (direction_distance, bfield_angles, bfield_phi):\n",
    "    x,y = direction_distance, bfield_angles\n",
    "    a = ( 1./(x[...,2]-x[...,0]) ) * ( (y[...,2]-y[...,1])/(x[...,2]-x[...,1]) - (y[...,1]-y[...,0])/(x[...,1]-x[...,0]) )\n",
    "    b = (y[...,1]-y[...,0])/(x[...,1]-x[...,0]) - a*(x[...,0]-x[...,1])\n",
    "    c = y[...,0] - a*x[...,0]**2 - b*x[...,0]\n",
    "    bfield_rotation = c\n",
    "    #del x,y, a,b\n",
    "    # we also need a length scale, we take it from the square-neighbourhood of our bfield\n",
    "    vectors = npstack((\n",
    "        cos(bfield_phi), sin(bfield_phi)\n",
    "    ))\n",
    "    # normalize so that one coordinate ==1 (i.e., we're on a unit square)\n",
    "    vectors /= where(\n",
    "        vectors[0] > vectors[1],\n",
    "        vectors[0],\n",
    "        vectors[1]\n",
    "    )\n",
    "    length = sqrt(npsum(\n",
    "        vectors**2, axis=0\n",
    "    ))\n",
    "    #del vectors\n",
    "    # calculate curvature radius\n",
    "    curvature = bfield_rotation / length\n",
    "    \n",
    "    # clean up and return\n",
    "    #del c, length \n",
    "    return transpose(curvature)\n",
    "\n",
    "def bfield_curvature (data, verbose=False):\n",
    "    # collect bfields as vectors\n",
    "    bfield = swapaxes(np.array((\n",
    "        data['Bcc1'],\n",
    "        data['Bcc2'],\n",
    "        #data['Bcc3'], # == 0\n",
    "    )), 0,-1)\n",
    "    if verbose: print('Calculating angles.. ', end='', flush=True)\n",
    "    # at each cell, collect angles between the cell's Bfield direction, and Bfield directions in the neighbouring cells\n",
    "    bfield_phi = _vector_orientation(bfield)[1:-1,1:-1]\n",
    "    direction_distance, bfield_angles = _bfield_angles(bfield, bfield_phi)\n",
    "    if verbose: print('done.', flush=True)\n",
    "    if verbose: print('Interpolating and extracting curvature.. ', end='', flush=True)\n",
    "    # now, interpolate these values as a function of position angle\n",
    "    # we'll use simple piecewise parabolic interpolation (y = ax2+bx+c)\n",
    "    curvature = _extract_bfield_curvature (direction_distance, bfield_angles, bfield_phi)\n",
    "    del bfield, direction_distance, bfield_angles\n",
    "    if verbose: print('done.', flush=True)\n",
    "    \n",
    "    # clean up and return\n",
    "    del bfield_phi\n",
    "    return curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNITS\n",
    "\n",
    "sim2phys = {\n",
    "    'Time':simu_t, # sec\n",
    "    'x1f':simu_len, # cm\n",
    "    'x2f':simu_len, # cm\n",
    "    'x3f':simu_len, # cm\n",
    "    'x1v':simu_len, # cm\n",
    "    'x2v':simu_len, # cm\n",
    "    'x3v':simu_len, # cm\n",
    "    'rho':simu_mass/simu_len**3, # g / cm^3\n",
    "    'press':simu_mass * (simu_len/simu_t)**2 / simu_len**3, # erg / cm^3\n",
    "    'vel1': 1.0, # c\n",
    "    'vel2': 1.0, # c\n",
    "    'vel3': 1.0, # c\n",
    "    'vel_tot': 1.0, # c\n",
    "    'Bcc1': simu_B, # sqrt(erg / cm^3)\n",
    "    'Bcc2': simu_B, # sqrt(erg / cm^3)\n",
    "    'Bcc3': simu_B, # sqrt(erg / cm^3)\n",
    "    'Bcc_tot': simu_B, # sqrt(erg / cm^3)\n",
    "    'Bcc_fluid_0': simu_B, # sqrt(erg / cm^3)\n",
    "    'Bcc_fluid_1': simu_B, # sqrt(erg / cm^3)\n",
    "    'Bcc_fluid_2': simu_B, # sqrt(erg / cm^3)\n",
    "    'Bcc_fluid_3': simu_B, # sqrt(erg / cm^3)\n",
    "    'Bcc_fluid_tot': simu_B, # sqrt(erg / cm^3)\n",
    "    'Bcc_fluid_tot_vsZ': simu_B, # sqrt(erg / cm^3)\n",
    "    'enthalpy': simu_mass * (simu_len/simu_t)**2 / simu_mass, # erg/g\n",
    "    'internal_energy': simu_mass * (simu_len/simu_t)**2 / simu_len**3, # erg / cm^3\n",
    "    'internal_energy_vsZ': simu_mass * (simu_len/simu_t)**2 / simu_len**3, # erg / cm^3\n",
    "    'ekin_observer': simu_mass * (simu_len/simu_t)**2 / simu_len**3, # erg / cm^3\n",
    "    'etot_observer': simu_mass * (simu_len/simu_t)**2 / simu_len**3, # erg / cm^3\n",
    "    'j_nu': 1.0, # erg / cm**3\n",
    "    'j_nu_vsZ': 1.0, # erg / cm**3\n",
    "    'j_over_alpha_nu': 1.0, # erg / cm**2\n",
    "    'j_over_alpha_nu_vsZ': 1.0, # erg / cm**2\n",
    "    'flux_density': 1.0, # erg/(cm**2*sec) / cm**2\n",
    "    'flux_density_vsZ': 1.0, # erg/(cm**2*sec) / cm**2\n",
    "    'syn_emission_rate_per_dS': 1.0, # erg/(cm**2*sec)\n",
    "    'spectrum': (1.0, 1.0), # (Hz, erg / (s cm**2 Hz) / cm**2)\n",
    "    'ddt_internal_energy': simu_mass * (simu_len/simu_t)**2 / simu_len**3 / simu_t, # erg / cm^3 / s\n",
    "    'ddt_internal_energy_vsZ': simu_mass * (simu_len/simu_t)**2 / simu_len**3 / simu_t, # erg / cm^3 / s\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_vertical_avg (data_vtk, quantity):\n",
    "    dy = (data_vtk['x2v'][1] - data_vtk['x2v'][0])\n",
    "    yrange = dy * len(data_vtk['x2v'])\n",
    "    data_vtk[quantity+'_vsZ'] = tf_deconvert(npsum(data_vtk[quantity]*dy, axis=0) / yrange)\n",
    "    \n",
    "default_augment_kwargs = {\n",
    "    'nu_res':128, 'nu_min':0.1*nu_int_min, 'nu_max':10.*nu_int_max,\n",
    "    'nu_int_min':nu_int_min, 'nu_int_max':nu_int_max,\n",
    "    'R_selection':R_choice, 'nu_selection':nu_choice,\n",
    "    'filling_factor':1.0\n",
    "}\n",
    "\n",
    "def augment_vtk_data (data_vtk, previous_data_vtk=None, \n",
    "                      nu_res=128, nu_min=1., nu_max=1.1*nu_int_max,\n",
    "                      nu_int_min=nu_int_min, nu_int_max=nu_int_max,\n",
    "                      R_selection=R_choice, nu_selection=nu_choice,\n",
    "                      filling_factor=1.0):\n",
    "    \n",
    "    ''' calculate auxiliary data '''\n",
    "    \n",
    "    xrange = (data_vtk['x1v'][1] - data_vtk['x1v'][0]) * len(data_vtk['x1v'])\n",
    "    yrange = (data_vtk['x2v'][1] - data_vtk['x2v'][0]) * len(data_vtk['x2v'])\n",
    "    \n",
    "    # SR quantities\n",
    "    # NOTE: in Athena 4.2 vel1, vel2, vel3 are 3-velocities. Instead Athena++ uses 4-velocities in the code (so they would need to be translated).\n",
    "    \n",
    "    # fluid velocities in collision frame\n",
    "    vel_tot_sqr = sqr_vec_l2(data_vtk['vel1'], data_vtk['vel2'], data_vtk['vel3'])\n",
    "    data_vtk['vel_tot'] = tf_deconvert(sqrt(vel_tot_sqr))\n",
    "    gam = vsqr2gamma(vel_tot_sqr)\n",
    "    data_vtk['gamma'] = tf_deconvert(gam)\n",
    "    del vel_tot_sqr\n",
    "    \n",
    "    # fluid velocities in observer frame (boosted by beta_jet)\n",
    "    beta_vec = moveaxis(np.array([data_vtk['vel1'], data_vtk['vel2'], data_vtk['vel3']]), 0,-1)\n",
    "    n_vec = np.array([cos(incl), 0, -sin(incl)])\n",
    "    combined_beta_vec, combined_beta, combined_gamma, doppler_factor = combined_doppler_factor(beta_vec, n_vec)\n",
    "    data_vtk['combined_beta_vec'] = tf_deconvert(combined_beta_vec)\n",
    "    data_vtk['combined_beta'] = tf_deconvert(combined_beta)\n",
    "    data_vtk['combined_gamma'] = tf_deconvert(combined_gamma)\n",
    "    data_vtk['doppler_factor'] = tf_deconvert(doppler_factor)\n",
    "\n",
    "    # total Bcc in collision frame\n",
    "    data_vtk['Bcc_tot'] = tf_deconvert(norm_vec_l2(data_vtk['Bcc1'], data_vtk['Bcc2'], data_vtk['Bcc3']))\n",
    "\n",
    "    # Bcc in the fluid frame\n",
    "    Bfl0 = BccFl0(\n",
    "        gam,\n",
    "        data_vtk['vel1'],data_vtk['vel2'],data_vtk['vel3'],\n",
    "        data_vtk['Bcc1'],data_vtk['Bcc2'],data_vtk['Bcc3'],\n",
    "    )\n",
    "    Bfl1 = BccFli(\n",
    "        gam, \n",
    "        data_vtk['vel1'], data_vtk['Bcc1'],\n",
    "        Bfl0\n",
    "    )\n",
    "    Bfl2 = BccFli(\n",
    "        gam, \n",
    "        data_vtk['vel2'], data_vtk['Bcc2'],\n",
    "        Bfl0\n",
    "    )\n",
    "    Bfl3 = BccFli(\n",
    "        gam, \n",
    "        data_vtk['vel3'], data_vtk['Bcc3'],\n",
    "        Bfl0\n",
    "    )\n",
    "    Bcc_fluid_tot_sqr = sqr_vec_l2(Bfl1, Bfl2, Bfl3)\n",
    "    Bcc_fluid_tot = sqrt(Bcc_fluid_tot_sqr)\n",
    "    Bcc_fluid_vec = moveaxis(np.array([Bfl1,Bfl2,Bfl3]),0,-1)\n",
    "    data_vtk['Bcc_fluid_0'] = tf_deconvert(Bfl0)\n",
    "    data_vtk['Bcc_fluid_1'] = tf_deconvert(Bfl1)\n",
    "    data_vtk['Bcc_fluid_2'] = tf_deconvert(Bfl2)\n",
    "    data_vtk['Bcc_fluid_3'] = tf_deconvert(Bfl3)\n",
    "    data_vtk['Bcc_fluid_tot'] = tf_deconvert(Bcc_fluid_tot)\n",
    "    do_vertical_avg(data_vtk, 'Bcc_fluid_tot')\n",
    "\n",
    "    # plasma parameters\n",
    "    data_vtk['plasma_beta'] = tf_deconvert(plasma_beta(data_vtk['press'], Bcc_fluid_tot_sqr))\n",
    "    data_vtk['magnetization'] = tf_deconvert(magnetization(Bcc_fluid_tot_sqr, data_vtk['rho']))\n",
    "    \n",
    "    # internal energy in the fluid frame\n",
    "    # see Beckwith & Stone (2011), https://github.com/PrincetonUniversity/athena/wiki/Special-Relativity\n",
    "    enth = enthalpy(adiab_idx, data_vtk['press'], data_vtk['rho'])\n",
    "    data_vtk['enthalpy'] = tf_deconvert(enth)\n",
    "    data_vtk['internal_energy'] = tf_deconvert(\n",
    "        internal_energy(data_vtk['rho'], enth, gam, data_vtk['press'], Bcc_fluid_tot_sqr)\n",
    "    ) # warning!: includes rest mass\n",
    "    do_vertical_avg(data_vtk, 'internal_energy')\n",
    "    data_vtk['ekin_observer'] = tf_deconvert(\n",
    "        ekin_observer(combined_gamma, data_vtk['rho'])\n",
    "    )\n",
    "    do_vertical_avg(data_vtk, 'ekin_observer')\n",
    "    data_vtk['etot_observer'] = tf_deconvert(\n",
    "        etot_observer(\n",
    "            beta_vec=combined_beta_vec, \n",
    "            beta_sqr=(combined_beta**2), \n",
    "            gamma=combined_gamma, \n",
    "            rho=data_vtk['rho'], \n",
    "            enthalpy=enth, \n",
    "            press=data_vtk['press'], \n",
    "            Bfluid_vec=Bcc_fluid_vec, \n",
    "            Bfluid_sqr=Bcc_fluid_tot_sqr\n",
    "        )\n",
    "    )\n",
    "    do_vertical_avg(data_vtk, 'etot_observer')\n",
    "    \n",
    "    del Bcc_fluid_tot_sqr, enth\n",
    "    \n",
    "    # synchrotron emission diagnostics\n",
    "    jnu = j_nu(nu2nu_fl(nu_selection,doppler_factor), Bcc_fluid_tot)\n",
    "    data_vtk['j_nu'] = tf_deconvert(jnu)\n",
    "    do_vertical_avg(data_vtk, 'j_nu')\n",
    "    alphanu = alpha_nu(nu2nu_fl(nu_selection,doppler_factor), Bcc_fluid_tot)\n",
    "    data_vtk['alpha_nu'] = tf_deconvert(alphanu)\n",
    "    do_vertical_avg(data_vtk, 'alpha_nu')\n",
    "    janu = j_over_alpha_nu(nu2nu_fl(nu_selection,doppler_factor), Bcc_fluid_tot)\n",
    "    data_vtk['j_over_alpha_nu'] = tf_deconvert(janu)\n",
    "    do_vertical_avg(data_vtk, 'j_over_alpha_nu')\n",
    "    flux_tot = flux_total_per_dS(\n",
    "        B=Bcc_fluid_tot, gamma=combined_gamma, df=doppler_factor,\n",
    "        R=R_selection, \n",
    "        nu_min=nu_int_min, nu_max=nu_int_max\n",
    "    )\n",
    "    data_vtk['flux_density'] = tf_deconvert(flux_tot)\n",
    "    do_vertical_avg(data_vtk, 'flux_density')\n",
    "    \n",
    "    nu_min, nu_max = tf_convert(nu_min, nu_max)\n",
    "    freqs = logspace(log10(nu_min), log10(nu_max), nu_res)\n",
    "    \n",
    "    dS = (data_vtk['x1v'][1] - data_vtk['x1v'][0]) * (data_vtk['x2v'][1] - data_vtk['x2v'][0])\n",
    "    if not low_memory:\n",
    "        nu_grid, B_grid = meshgrid(freqs, Bcc_fluid_tot, indexing='ij')\n",
    "        nu_grid, gamma_grid = meshgrid(freqs, combined_gamma, indexing='ij')\n",
    "        nu_grid, df_grid = meshgrid(freqs, doppler_factor, indexing='ij')\n",
    "        data_vtk['spectrum'] = [\n",
    "            tf_deconvert(freqs),\n",
    "            tf_deconvert(nansum(\n",
    "                flux_nu_per_dS(\n",
    "                    nu=nu_grid, \n",
    "                    B=B_grid,\n",
    "                    gamma=gamma_grid,\n",
    "                    doppler_factor=df_grid,\n",
    "                    R=R_selection, \n",
    "                    filling_factor=filling_factor\n",
    "                )*dS, axis=-1) / (xrange*yrange)\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        data_vtk['spectrum'] = [[],[]]\n",
    "        for nu in tf_deconvert(freqs):\n",
    "            data_vtk['spectrum'][0].append(nu)\n",
    "            data_vtk['spectrum'][1].append(tf_deconvert(\n",
    "                nansum(\n",
    "                    flux_nu_per_dS(\n",
    "                        nu=nu, B=Bcc_fluid_tot,\n",
    "                        gamma=combined_gamma,\n",
    "                        doppler_factor=doppler_factor,\n",
    "                        R=R_selection,\n",
    "                        filling_factor=filling_factor\n",
    "                    )\n",
    "                ) * dS / (xrange*yrange)\n",
    "            ))\n",
    "        data_vtk['spectrum'] = [np.array(data_vtk['spectrum'][0]), np.array(data_vtk['spectrum'][1])]\n",
    "    \n",
    "    # time derivatives\n",
    "    if type(previous_data_vtk) is dict:\n",
    "        for quantity in ['internal_energy','internal_energy_vsZ']:\n",
    "            data_vtk['ddt_'+quantity] = (data_vtk[quantity]-previous_data_vtk[quantity]) / (data_vtk['Time']-previous_data_vtk['Time'])\n",
    "            \n",
    "    # magnetic field curvature\n",
    "    data_vtk['curvature'] = bfield_curvature(data_vtk)\n",
    "    \n",
    "    # Stokes parameters\n",
    "    Bfluid_vec = moveaxis([Bfl1,Bfl2,Bfl3],0,-1)\n",
    "    qprime_vec = stokes_qprime(\n",
    "        Bfluid_vec=Bfluid_vec, \n",
    "        beta_vec=combined_beta_vec, \n",
    "        gamma=combined_gamma,\n",
    "        n_vec=n_vec\n",
    "    )\n",
    "    e_vec = stokes_e (\n",
    "        n_vec=n_vec, \n",
    "        qprime_vec=qprime_vec\n",
    "    )\n",
    "    l_vec = np.array([sin(incl),0,cos(incl)])\n",
    "    cos_xitilde = stokes_cos_xitilde(\n",
    "        e_vec=e_vec, \n",
    "        n_vec=n_vec, \n",
    "        l_vec=l_vec\n",
    "    )\n",
    "    sin_xitilde = stokes_sin_xitilde(\n",
    "        e_vec=e_vec,\n",
    "        l_vec=l_vec\n",
    "    )\n",
    "    sin_loc_incl = sin_local_incl(\n",
    "        beta_vec=combined_beta_vec,\n",
    "        beta=combined_beta,\n",
    "        n_vec=n_vec\n",
    "    )\n",
    "    nprime_vec = stokes_nprime(\n",
    "        n_vec=n_vec, \n",
    "        gamma=combined_gamma, \n",
    "        beta_vec=combined_beta_vec\n",
    "    )\n",
    "    sin_xiprime = stokes_sin_xiprime(\n",
    "        nprime_vec=nprime_vec,\n",
    "        Bfluid_vec=Bfluid_vec,\n",
    "        Bfluid=Bcc_fluid_tot\n",
    "    )\n",
    "    data_vtk['stokes_I'] = tf_deconvert(stokes_I(\n",
    "        sin_local_incl=sin_loc_incl, \n",
    "        doppler_factor=doppler_factor, \n",
    "        Bfluid=Bcc_fluid_tot, \n",
    "        sin_xiprime=sin_xiprime\n",
    "    ))\n",
    "    data_vtk['stokes_Q'] = tf_deconvert(stokes_Q(\n",
    "        sin_local_incl=sin_loc_incl, \n",
    "        doppler_factor=doppler_factor, \n",
    "        Bfluid=Bcc_fluid_tot, \n",
    "        sin_xiprime=sin_xiprime, \n",
    "        cos_xitilde=cos_xitilde\n",
    "    ))\n",
    "    data_vtk['stokes_U'] = tf_deconvert(stokes_U(\n",
    "        sin_local_incl=sin_loc_incl, \n",
    "        doppler_factor=doppler_factor, \n",
    "        Bfluid=Bcc_fluid_tot, \n",
    "        sin_xiprime=sin_xiprime, \n",
    "        sin_xitilde=sin_xitilde, \n",
    "        cos_xitilde=cos_xitilde\n",
    "    ))\n",
    "    del Bfluid_vec, qprime_vec, e_vec, l_vec, cos_xitilde, sin_xitilde, sin_loc_incl, nprime_vec, sin_xiprime, \n",
    "    \n",
    "    return data_vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_vtk_file (vtk_filename, previous_data_vtk=None, out_dt=out_dt_vtk, augment_kwargs=default_augment_kwargs, tarpath=None):\n",
    "    '''read and augment the data'''\n",
    "    # check if tarpath has been extracted\n",
    "    if tarpath != None and os.path.isdir('.'.join(tarpath.split('.')[:-1])):\n",
    "        print('[read_vtk_file(%s)] tarpath already extracted. Using the extracted version.' % tarpath)\n",
    "        vtk_filename = '.'.join(tarpath.split('.')[:-1]) + '/' + '/'.join(vtk_filename.split('/')[1:])\n",
    "        tarpath = None\n",
    "    if os.path.isfile(vtk_filename + '.pkl') or (tarpath != None and (vtk_filename+'.pkl' in [x.path for x in tarfile.open(tarpath).getmembers()])):\n",
    "        vtk_filename += '.pkl'\n",
    "    if '.pkl' in vtk_filename:\n",
    "        if tarpath == None:\n",
    "            with open(vtk_filename, 'rb') as f:\n",
    "                data_vtk, augment_kwargs_loaded = pkl.load(f)\n",
    "        else:\n",
    "            tar = tarfile.open(tarpath)\n",
    "            data_vtk = tar.extractfile(vtk_filename)\n",
    "            data_vtk, augment_kwargs_loaded = pkl.load(data_vtk)\n",
    "            tar.close()\n",
    "        # recalculate augmentation if needed\n",
    "        if force_recalc or augment_kwargs_loaded != augment_kwargs:\n",
    "            data_vtk = augment_vtk_data(data_vtk, previous_data_vtk=previous_data_vtk, **augment_kwargs)\n",
    "            if convert_vtk:\n",
    "                if tarpath == None:\n",
    "                    with open(vtk_filename,'wb') as f:\n",
    "                        pkl.dump((data_vtk, augment_kwargs), f)\n",
    "                else:\n",
    "                    print('[read_vtk_file(%s)]: cannot edit tar archives to save *.pkl files, please extract the archive first. Continuing without saving the *.pkl file.' % vtk_filename)\n",
    "    else:\n",
    "        if tarpath == None:\n",
    "            data_vtk = augment_vtk_data(vtk(vtk_filename, out_dt=out_dt_vtk), previous_data_vtk=previous_data_vtk, **augment_kwargs)\n",
    "        else:\n",
    "            tar = tarfile.open(tarpath, 'r')\n",
    "            data_vtk = tar.extractfile(vtk_filename).read()\n",
    "            data_vtk = augment_vtk_data(vtk(filename=vtk_filename, out_dt=1.0, input_string=data_vtk))\n",
    "            tar.close()\n",
    "        if convert_vtk:\n",
    "            if tarpath == None:\n",
    "                with open(vtk_filename + '.pkl','wb') as f:\n",
    "                    pkl.dump((data_vtk, augment_kwargs), f)\n",
    "                try: # if successful, remove the original vtk file\n",
    "                    with open(vtk_filename + '.pkl','rb') as f:\n",
    "                        _ = pkl.load(f)\n",
    "                    del _\n",
    "                    os.remove(vtk_filename)\n",
    "                except Exception as e:\n",
    "                    print(' - could not save a pkl from %s\\n%s' % (vtk_filename, e))\n",
    "            else:\n",
    "                print('[read_vtk_file(%s)]: cannot edit tar archives to save *.pkl files, please extract the archive first. Continuing without saving the *.pkl file.' % vtk_filename)\n",
    "    return data_vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_quantities = ['times', 'internal_energy', 'flux_density', 'syn_emission_rate_per_dS', 'ekin_observer', 'etot_observer', 'polarization_degree', 'polarization_evpa']\n",
    "\n",
    "def _precalc_history_batch (vtk_filenames, out_dt=out_dt_vtk, augment_kwargs=default_augment_kwargs, tarpath=None):\n",
    "    previous_data_vtk = None\n",
    "    history = {}\n",
    "    for quantity in history_quantities:\n",
    "        history[quantity] = []\n",
    "    for vtk_filename in vtk_filenames:\n",
    "        fileno = int(vtk_filename.split('/')[-1].split('.')[1])\n",
    "        # read and augment the data\n",
    "        try:\n",
    "            data_vtk = read_vtk_file(vtk_filename, previous_data_vtk, out_dt=out_dt, augment_kwargs=augment_kwargs, tarpath=tarpath)\n",
    "        except Exception as e:\n",
    "            print('[precalc_history] Could not read vtk file %s, error occured:' % vtk_filename)\n",
    "            print(e)\n",
    "            print(' - the file will be ignored.')\n",
    "            continue\n",
    "        # calculate history variables\n",
    "        history['times'].append(fileno*out_dt)\n",
    "        # energy components\n",
    "        history['internal_energy'].append(tf_deconvert(\n",
    "            npmean(data_vtk['internal_energy_vsZ'])\n",
    "        ))\n",
    "        history['ekin_observer'].append(tf_deconvert(\n",
    "            npmean(data_vtk['ekin_observer_vsZ'])\n",
    "        ))\n",
    "        history['etot_observer'].append(tf_deconvert(\n",
    "            npmean(data_vtk['etot_observer_vsZ'])\n",
    "        ))\n",
    "        history['flux_density'].append(tf_deconvert(\n",
    "            npmean(data_vtk['flux_density_vsZ'])\n",
    "        ))\n",
    "        # synchrotron emission\n",
    "        history['syn_emission_rate_per_dS'].append(get_cgs_value(tf_deconvert(\n",
    "            syn_emission_rate_per_dS(data_vtk['flux_density'], doppler_factor=data_vtk['doppler_factor'])\n",
    "        )))\n",
    "        # polarization from the Stokes parameters\n",
    "        Q_integral = tf_deconvert(nansum(data_vtk['stokes_Q']))\n",
    "        U_integral = tf_deconvert(nansum(data_vtk['stokes_U']))\n",
    "        I_integral = tf_deconvert(nansum(data_vtk['stokes_I']))\n",
    "        history['polarization_degree'].append(tf_deconvert(\n",
    "            sqrt(Q_integral**2 + U_integral**2) / I_integral\n",
    "        ))\n",
    "        history['polarization_evpa'].append(tf_deconvert(\n",
    "            0.5 * np.arccos(Q_integral / sqrt(Q_integral**2 + U_integral**2))\n",
    "        ))\n",
    "        del Q_integral, U_integral, I_integral\n",
    "        # move on\n",
    "        del previous_data_vtk\n",
    "        previous_data_vtk = data_vtk\n",
    "    for quantity in history_quantities:\n",
    "        history[quantity] = np.array(history[quantity])\n",
    "    history['ddt_internal_energy'] = (history['internal_energy'][1:] - history['internal_energy'][:-1]) / (history['times'][1:] - history['times'][:-1])\n",
    "    history['ddt_internal_energy'] = np.insert(history['ddt_internal_energy'], 0, np.nan)\n",
    "    return history\n",
    "\n",
    "def precalc_history (vtk_filenames, out_dt=out_dt_vtk, augment_kwargs=default_augment_kwargs, tarpath=None):\n",
    "    print('Calculating history..')\n",
    "    if nproc_history == 1:\n",
    "        history = _precalc_history_batch(vtk_filenames, out_dt=out_dt, augment_kwargs=augment_kwargs, tarpath=tarpath)\n",
    "    else:\n",
    "        from pathos.pools import ProcessPool # an alternative to Python's multiprocessing\n",
    "        history = {}\n",
    "        # prepare batches for parallel processing\n",
    "        batches = np.array_split(vtk_filenames, nproc_history)\n",
    "        for i in range(nproc_history-1):\n",
    "            batches[i+1] = np.insert(batches[i+1], 0, batches[i][-1])\n",
    "        # process batch histories\n",
    "        with ProcessPool(nproc_history) as pool:\n",
    "            batch_histories = pool.map(\n",
    "                lambda x : _precalc_history_batch(\n",
    "                    x, out_dt=out_dt, augment_kwargs=augment_kwargs, tarpath=tarpath\n",
    "                ), \n",
    "                batches\n",
    "            )\n",
    "        # combine batch histories\n",
    "        for batch_history in batch_histories:\n",
    "            for quantity in batch_history.keys():\n",
    "                if quantity not in history.keys():\n",
    "                    history[quantity] = batch_history[quantity]\n",
    "                else:\n",
    "                    history[quantity] = np.append(history[quantity], batch_history[quantity][1:])\n",
    "        del batch_histories\n",
    "    print(' - history calculation done.')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "# Single-dataset dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if processing_type == 'dashboard':\n",
    "    \n",
    "    # check if we are reading from a folder or from a tarfile\n",
    "    if ('.tgz' in datapath) and os.path.isdir(datapath[:-4]):\n",
    "        print('Tarfile %s has already been extracted, using the extracted version.' % datapath)\n",
    "        datapath = datapath[:-4]+'/'\n",
    "    using_tarfile = ('.tgz' in datapath)\n",
    "\n",
    "    if not using_tarfile:\n",
    "        vtk_filenames = sorted(list(set(\n",
    "            glob.glob(datapath + 'joined_vtk/*.vtk') + [x[:-4] for x in glob.glob(datapath + 'joined_vtk/*.vtk.pkl')]\n",
    "        )))\n",
    "    else:\n",
    "        tar = tarfile.open(datapath)\n",
    "        vtk_filenames = sorted(\n",
    "            [x.path for x in tar.getmembers() if (x.path[-4:] == '.vtk')] + [x.path[:-4] for x in tar.getmembers() if (x.path[-8:] == '.vtk.pkl')]\n",
    "        )\n",
    "        tar.close()\n",
    "\n",
    "    outpath = './temp_dashboard/'\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "\n",
    "    n_levels = 64\n",
    "    minmax_kwargs = {\n",
    "        'plasma_beta': [1.0e-4, 1.0e4]\n",
    "    }\n",
    "\n",
    "    # WARNING: add_snapshot_FIFO below is NOT embarassingly parallelizable, but does significantly save memory\n",
    "    def dashboard_frame (i_vtk, verbose=False, save=True, recalculate=False, previous_data_vtk=None, history=None, augment_kwargs=default_augment_kwargs, tarpath=None):\n",
    "\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.gridspec as gridspec\n",
    "\n",
    "        from read_vtk import vtk\n",
    "\n",
    "        fileno = int(vtk_filenames[i_vtk].split('/')[-1].split('.')[1])\n",
    "        vtk_time = fileno * out_dt_vtk # presumably we can trust this..\n",
    "\n",
    "        print('Processing vtk no %i, vtk_time = %.2e..' % (i_vtk, vtk_time), flush=True)\n",
    "\n",
    "        # see whether we can skip the file as already plotted\n",
    "        outfile = (outpath + 'dashboard_%05i.png') % fileno\n",
    "        if save and not recalculate and os.path.exists(outfile):\n",
    "            print(' - file already plotted, skipping.', flush=True)\n",
    "            return\n",
    "\n",
    "        # initialize the plot\n",
    "        fig = plt.figure(figsize=(24,12))\n",
    "        gs = gridspec.GridSpec(4,8, width_ratios=[1,0.1,0.1,1,0.1,0.1,1,0.1])\n",
    "        plt.suptitle('Time = %.2f sec.' % (vtk_time * sim2phys['Time'],))\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        if verbose:\n",
    "            print(' - processing vtk data..', flush=True)\n",
    "\n",
    "        # load, process, and plot the vtk file (mhd quantities)\n",
    "\n",
    "        if previous_data_vtk == None and i_vtk > 0:\n",
    "            previous_data_vtk = read_vtk_file(vtk_filenames[i_vtk-1], previous_data_vtk=None, out_dt=out_dt_vtk, augment_kwargs=augment_kwargs, tarpath=tarpath)\n",
    "\n",
    "        data_vtk = read_vtk_file(vtk_filenames[i_vtk], previous_data_vtk=previous_data_vtk, out_dt=out_dt_vtk, augment_kwargs=augment_kwargs, tarpath=tarpath)\n",
    "\n",
    "        # plot\n",
    "\n",
    "        plt.subplot(gs[0,0])\n",
    "        plt.contourf(data_vtk['x1v'], data_vtk['x2v'], data_vtk['magnetization'], levels=n_levels)\n",
    "        plt.gca().axes.xaxis.set_ticklabels([])\n",
    "        plt.title('Magnetization (fl. frame)')\n",
    "        plt.ylabel('y [lt-sec]')\n",
    "        plt.colorbar(cax=plt.subplot(gs[0,1]))\n",
    "\n",
    "        plt.subplot(gs[1,0])\n",
    "        plt.contourf(data_vtk['x1v'], data_vtk['x2v'], data_vtk['Bcc_fluid_tot'] * sim2phys['Bcc_fluid_tot'], levels=n_levels)\n",
    "        plt.title('Bfield (fl. frame) [ ( erg $/$ cm$^3 $)$^{1/2}$ ]')\n",
    "        plt.ylabel('y [lt-sec]')\n",
    "        plt.gca().axes.xaxis.set_ticklabels([])\n",
    "        plt.colorbar(cax=plt.subplot(gs[1,1]))\n",
    "\n",
    "        ax = plt.subplot(gs[2,0])\n",
    "        plt.plot(data_vtk['x1v'], data_vtk['Bcc_fluid_tot_vsZ'] * sim2phys['Bcc_fluid_tot_vsZ'], 'k-')\n",
    "        plt.gca().axes.xaxis.set_ticklabels([])\n",
    "        plt.ylabel('y [lt-sec]')\n",
    "        plt.title('Bfield (fl. frame) [ ( erg $/$ cm$^3 $)$^{1/2}$ ]')\n",
    "        ax.set_xlim(min(data_vtk['x1v']), max(data_vtk['x1v']))\n",
    "\n",
    "        ax = plt.subplot(gs[3,0])\n",
    "        plt.contourf(data_vtk['x1v'], data_vtk['x2v'], \n",
    "                     np.where(np.logical_and(data_vtk['plasma_beta'] > minmax_kwargs['plasma_beta'][0],\n",
    "                                             data_vtk['plasma_beta'] < minmax_kwargs['plasma_beta'][1]), \n",
    "                              np.log10(data_vtk['plasma_beta']), np.nan),\n",
    "                     levels=n_levels)\n",
    "        plt.title('Log$_{10}$ Plasma $\\\\beta$')\n",
    "        plt.ylabel('y [lt-sec]')\n",
    "        plt.xlabel('x [lt-sec]')\n",
    "        plt.colorbar(cax=plt.subplot(gs[3,1]))\n",
    "        ax.set_xlim(min(data_vtk['x1v']), max(data_vtk['x1v']))\n",
    "        ax.set_ylim(min(data_vtk['x2v']), max(data_vtk['x2v']))\n",
    "\n",
    "        plt.subplot(gs[0,3])\n",
    "        plt.contourf(data_vtk['x1v'], data_vtk['x2v'], data_vtk['rho'] * sim2phys['rho'], levels=n_levels)\n",
    "        plt.title('Density (fl. frame) [ g $/$ cm$^3$ ]')\n",
    "        plt.gca().axes.xaxis.set_ticklabels([])\n",
    "        plt.ylabel('y [lt-sec]')\n",
    "        plt.colorbar(cax=plt.subplot(gs[0,4]))\n",
    "\n",
    "        plt.subplot(gs[1,3])\n",
    "        plt.contourf(data_vtk['x1v'], data_vtk['x2v'], data_vtk['flux_density'] * sim2phys['flux_density'], levels=n_levels)\n",
    "        plt.title('Syn. flux / dS [ erg $/$ (cm$^2$ s) $/$ cm$^2$ ]')\n",
    "        plt.gca().axes.xaxis.set_ticklabels([])\n",
    "        plt.ylabel('y [lt-sec]')\n",
    "        plt.colorbar(cax=plt.subplot(gs[1,4]))\n",
    "\n",
    "        ax = plt.subplot(gs[2,3])\n",
    "        plt.plot(data_vtk['x1v'], data_vtk['j_nu_vsZ'] * sim2phys['j_nu_vsZ'], 'k-')\n",
    "        plt.gca().axes.xaxis.set_ticklabels([])\n",
    "        plt.ylabel('$j_{\\\\nu}(1.4$ PHz$)$ [ erg $/$ cm$^3$ ]')\n",
    "        ax.set_xlim(min(data_vtk['x1v']), max(data_vtk['x1v']))\n",
    "        ax2 = plt.twinx()\n",
    "        plt.plot(data_vtk['x1v'], data_vtk['j_over_alpha_nu_vsZ'] * sim2phys['j_over_alpha_nu_vsZ'], 'b-')\n",
    "        plt.ylabel('$j_{\\\\nu}/\\\\alpha_{\\\\nu}(1.4$ PHz$)$ [ erg $/$ cm$^2$ ]')\n",
    "        ax2.spines['right'].set_color('b')\n",
    "        ax2.tick_params(axis='y', which='both', colors='b')\n",
    "        ax2.yaxis.label.set_color('b')\n",
    "\n",
    "        plt.subplot(gs[0,6])\n",
    "        plt.contourf(data_vtk['x1v'], data_vtk['x2v'], data_vtk['internal_energy'] * sim2phys['internal_energy'], levels=n_levels)\n",
    "        plt.title('Internal energy, $U$ (fl. frame) [ erg $/$ cm$^3$ ]')\n",
    "        plt.gca().axes.xaxis.set_ticklabels([])\n",
    "        plt.ylabel('y [lt-sec]')\n",
    "        plt.colorbar(cax=plt.subplot(gs[0,7]))\n",
    "\n",
    "        if 'ddt_internal_energy' in data_vtk.keys():\n",
    "            plt.subplot(gs[1,6])\n",
    "            plt.contourf(data_vtk['x1v'], data_vtk['x2v'], data_vtk['ddt_internal_energy'] * sim2phys['ddt_internal_energy'], levels=n_levels)\n",
    "            plt.title('d(Internal energy)/dt, $\\\\dot{U}$ (fl. frame) [ erg $/$ (cm$^3$ s) ]')\n",
    "            plt.gca().axes.xaxis.set_ticklabels([])\n",
    "            plt.ylabel('y [lt-sec]')\n",
    "            plt.colorbar(cax=plt.subplot(gs[1,7]))\n",
    "\n",
    "            ax = plt.subplot(gs[2,6])\n",
    "            plt.plot(data_vtk['x1v'], data_vtk['internal_energy_vsZ'] * sim2phys['internal_energy_vsZ'], 'k-')\n",
    "            plt.gca().axes.xaxis.set_ticklabels([])\n",
    "            plt.ylabel('$U$ [ erg $/$ cm$^3$ ]')\n",
    "            ax.set_xlim(min(data_vtk['x1v']), max(data_vtk['x1v']))\n",
    "            ax2 = plt.twinx()\n",
    "            plt.plot(data_vtk['x1v'], data_vtk['ddt_internal_energy_vsZ'] * sim2phys['ddt_internal_energy_vsZ'], 'b-')\n",
    "            plt.ylabel('$\\\\dot{U}_{\\\\rm avg}$ [ erg $/$ (cm$^3$ s) ]')\n",
    "            ax2.spines['right'].set_color('b')\n",
    "            ax2.tick_params(axis='y', which='both', colors='b')\n",
    "            ax2.yaxis.label.set_color('b')\n",
    "\n",
    "        ax = plt.subplot(gs[3,6])\n",
    "        plt.plot(\n",
    "            data_vtk['spectrum'][0] * sim2phys['spectrum'][0], \n",
    "            data_vtk['spectrum'][1] * sim2phys['spectrum'][1], 'k-')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        plt.xlabel('$\\\\nu$ [Hz]')\n",
    "        plt.ylabel('$F_{\\\\nu\\\\rm,syn}$ [ erg $/$ (s cm$^2$ Hz) $/$ cm$^2$ ]')\n",
    "        plt.axvline(augment_kwargs['nu_int_min'], color='k', ls=':')\n",
    "        plt.axvline(augment_kwargs['nu_int_max'], color='k', ls=':')\n",
    "\n",
    "        # set the plot range to vtk data\n",
    "        for j in range(2):\n",
    "            for i in range(3):\n",
    "                ax = plt.subplot(gs[j,3*i])\n",
    "                ax.set_xlim(min(data_vtk['x1v']), max(data_vtk['x1v']))\n",
    "                ax.set_ylim(min(data_vtk['x2v']), max(data_vtk['x2v']))\n",
    "\n",
    "        # plot the history data\n",
    "        if type(history) == dict:\n",
    "            ax = plt.subplot(gs[3,3])\n",
    "            mask = history['times'] < vtk_time\n",
    "            plt.plot(\n",
    "                history['times'][mask] * sim2phys['Time'], \n",
    "                history['ddt_internal_energy'][mask] * sim2phys['ddt_internal_energy'], 'r-')\n",
    "            mask = history['times'] == vtk_time\n",
    "            plt.scatter(\n",
    "                history['times'][mask] * sim2phys['Time'], \n",
    "                history['ddt_internal_energy'][mask] * sim2phys['ddt_internal_energy'], color='r', s=8)\n",
    "            mask = history['times'] > vtk_time\n",
    "            plt.plot(\n",
    "                history['times'][mask] * sim2phys['Time'], \n",
    "                history['ddt_internal_energy'][mask] * sim2phys['ddt_internal_energy'], 'r:')\n",
    "            plt.ylabel('$\\dot{U}_{\\\\rm avg}$ [erg $/$ (cm$^3$ s)]')\n",
    "            plt.xlabel('Time [sec]')\n",
    "            ax.spines['left'].set_color('r')\n",
    "            ax.tick_params(axis='y', which='both', colors='r')\n",
    "            ax.yaxis.label.set_color('r')\n",
    "            ax2 = plt.twinx()\n",
    "            mask = history['times'] < vtk_time\n",
    "            plt.plot(\n",
    "                history['times'][mask] * sim2phys['Time'],\n",
    "                history['flux_density'][mask] * sim2phys['flux_density'], 'b-')\n",
    "            mask = history['times'] == vtk_time\n",
    "            plt.scatter(\n",
    "                history['times'][mask] * sim2phys['Time'],\n",
    "                history['flux_density'][mask] * sim2phys['flux_density'], color='b', s=8)\n",
    "            mask = history['times'] > vtk_time\n",
    "            plt.plot(\n",
    "                history['times'][mask] * sim2phys['Time'],\n",
    "                history['flux_density'][mask] * sim2phys['flux_density'], 'b:')\n",
    "            plt.ylabel('Avg. syn. flux / dS [${\\\\rm erg}/({\\\\rm cm}^2{\\\\rm s}) / {\\\\rm cm}^2$]')\n",
    "            ax2.spines['right'].set_color('b')\n",
    "            ax2.tick_params(axis='y', which='both', colors='b')\n",
    "            ax2.yaxis.label.set_color('b')\n",
    "\n",
    "        # clean up\n",
    "        del data_vtk\n",
    "\n",
    "        if verbose:\n",
    "            print('     done.', flush=True)\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        if save:\n",
    "            plt.savefig(outfile, format='png', dpi=300, facecolor='w')\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        print(' - frame done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if processing_type == 'dashboard':\n",
    "    if '.tgz' not in datapath:\n",
    "        history_outfile = datapath+'history.pkl'\n",
    "        if force_recalc or not os.path.exists(history_outfile):\n",
    "            history = precalc_history(vtk_filenames, out_dt=out_dt_vtk)\n",
    "            with open(history_outfile, 'wb') as f:\n",
    "                pkl.dump(history, f)\n",
    "            force_recalc = False # recalc done\n",
    "        else:\n",
    "            with open(history_outfile, 'rb') as f:\n",
    "                history = pkl.load(f)\n",
    "    else:\n",
    "        tar = tarfile.open(datapath)\n",
    "        extract = False\n",
    "        history_outfile = ''.join([x.path if ('history.pkl' in x.path) else '' for x in tar.getmembers()])\n",
    "        if history_outfile == '': # create the history file\n",
    "            \n",
    "            # is we need to save *.pkl files and we are using tarfile, it needs to be extracted to be edited\n",
    "            print('Extracting tarfile %s to add history.pkl.. ' % datapath, end='')\n",
    "            extract = True\n",
    "            tar.extractall(path = '/'.join(datapath.split('/')[:-1]))\n",
    "            datapath = datapath[:-4]+'/'\n",
    "            vtk_filenames = [datapath+'/'.join(x.split('/')[1:]) for x in vtk_filenames]\n",
    "            print('done.')\n",
    "        \n",
    "            # calculate the history\n",
    "            history = precalc_history(vtk_filenames, out_dt=out_dt_vtk)\n",
    "            # save the history\n",
    "            history_outfile = datapath+'history.pkl'\n",
    "            with open(history_outfile, 'wb') as f:\n",
    "                pkl.dump(history, f)\n",
    "            force_recalc = False # recalc done\n",
    "        else:\n",
    "            history = pkl.load(tar.extractfile(history_outfile))\n",
    "        tar.close()\n",
    "        \n",
    "        # remove the tarfile if extracted\n",
    "        if extract:\n",
    "            os.remove(datapath[:-1]+'.tgz')\n",
    "            datapath = datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if processing_type == 'dashboard':\n",
    "    # is we need to save *.pkl files and we are using tarfile, it needs to be extracted to be edited\n",
    "    extract = False\n",
    "    tarpath = None\n",
    "    if '.tgz' in datapath:\n",
    "        if os.path.isdir('.'.join(datapath.split('.')[:-1])): # already extracted\n",
    "            datapath = '.'.join(datapath.split('.')[:-1])\n",
    "            tarpath = None\n",
    "        elif convert_vtk: # check if we need to extract to convert vtk to pkl\n",
    "            contents = [(1 if x.path[:-4] == '.vtk' else 0) for x in tarfile.open(datapath).getmembers()]\n",
    "            extract = (np.sum(contents) > 0)\n",
    "            if extract:\n",
    "                print('Extracting tarfile %s.. ' % datapath, end='')\n",
    "                tar = tarfile.open(datapath)\n",
    "                tar.extractall(path = '.'.join(datapath.split['.'][:-1]))\n",
    "                datapath = datapath[:-4]+'/'\n",
    "                vtk_filenames = [datapath+'/'.join(x.split('/')[1:]) for x in vtk_filenames]\n",
    "                tar.close()\n",
    "                tarpath = None\n",
    "                print('done.')\n",
    "            else:\n",
    "                tarpath = datapath\n",
    "        else: # keep reading from the compressed tarfile\n",
    "            tarpath = datapath\n",
    "    # now, parallelize the frame generation\n",
    "    from pathos.pools import ProcessPool # an alternative to Python's multiprocessing\n",
    "    chunks = np.array_split(range(len(vtk_filenames)), nproc)\n",
    "    def worker (ichunk):\n",
    "        indices = chunks[ichunk]\n",
    "        data_vtk = None\n",
    "        for i in indices:\n",
    "            data_vtk = dashboard_frame(i_vtk=i, recalculate=force_recalc, previous_data_vtk=data_vtk, history=history, augment_kwargs=default_augment_kwargs, verbose=False, tarpath=tarpath)\n",
    "    with ProcessPool(nproc) as pool:\n",
    "        _ = pool.map(worker, list(range(nproc)))\n",
    "    # tar when done and clean up\n",
    "    if extract or (tar_when_done and '.tgz' not in datapath):\n",
    "            print('Archiving into tarfile %s.. ' % datapath, end='')\n",
    "            workdir = os.getcwd()\n",
    "            tar_location = '/'.join(datapath.split('/')[:-2])\n",
    "            if tar_location != '':\n",
    "                os.chdir(tar_location)\n",
    "            os.system('tar -cvzf %s %s' % (datapath.split('/')[-2] + '.tgz',datapath.split('/')[-2]))\n",
    "            if os.path.isfile(datapath[:-1] + '.tgz'):\n",
    "                rmtree(datapath)\n",
    "                datapath = datapath[:-1] + '.tgz'\n",
    "                tarpath = datapath\n",
    "            os.chdir(workdir)\n",
    "            print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if processing_type == 'dashboard':\n",
    "    # render the movie\n",
    "    try:\n",
    "        print(\"Rendering the movie..\", flush=True)\n",
    "        command = (\"ffmpeg -threads %i -y -r 20 -f image2 -i \\\"%sdashboard_%%*.png\\\" -f mp4 -q:v 0 -vcodec mpeg4 -r 20 dashboard.mp4\" % (nproc, outpath,))\n",
    "        print(command, flush=True)\n",
    "        os.system(command)\n",
    "        if os.path.isfile('dashboard.mp4'):\n",
    "            os.system('rm -r ' + outpath)\n",
    "        else:\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        print('Error while rendering movie:\\n%s\\n -- please try to manually convert the .png files generated in %s.' % (e, outpath), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False and processing_type == 'dashboard' and not in_script:\n",
    "    dashboard_frame(80, save=False, recalculate=False, history=history, tarpath=tarpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if processing_type == 'dashboard':\n",
    "    print(\"DASHBOARD PROCESSING DONE.\", flush=True)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "# Two-dataset comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:10<00:00,  2.66s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/ppjanka/anaconda3/envs/intsh2/lib/python3.6/site-packages/ipykernel_launcher.py:62: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/ppjanka/anaconda3/envs/intsh2/lib/python3.6/site-packages/ipykernel_launcher.py:62: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/ppjanka/anaconda3/envs/intsh2/lib/python3.6/site-packages/ipykernel_launcher.py:62: RuntimeWarning: invalid value encountered in arccos\n",
      " 25%|       | 1/4 [08:13<24:39, 493.31s/it]/home/ppjanka/anaconda3/envs/intsh2/lib/python3.6/site-packages/ipykernel_launcher.py:62: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/ppjanka/anaconda3/envs/intsh2/lib/python3.6/site-packages/ipykernel_launcher.py:62: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/ppjanka/anaconda3/envs/intsh2/lib/python3.6/site-packages/ipykernel_launcher.py:62: RuntimeWarning: invalid value encountered in arccos\n",
      " 50%|     | 2/4 [16:48<16:52, 506.23s/it]"
     ]
    }
   ],
   "source": [
    "if processing_type == 'comparison':\n",
    "    \n",
    "    def diff_name(a,b):\n",
    "        \"\"\"Returns a comparison-style name based on two strings\"\"\"\n",
    "        # don't edit the original paths\n",
    "        a,b = copy(a), copy(b)\n",
    "        # strip the .tgz, if needed\n",
    "        if a[-4:] == '.tgz': a = a[:-4]\n",
    "        if b[-4:] == '.tgz': b = b[:-4]\n",
    "        # strip the trailing /\n",
    "        if a[-1] == '/': a = a[:-1]\n",
    "        if b[-1] == '/': b = b[:-1]\n",
    "        # first, read from the front\n",
    "        idxl = 0\n",
    "        while a[:idxl] == b[:idxl]:\n",
    "            idxl += 1\n",
    "        # then, read from the back\n",
    "        idxr = 1\n",
    "        while a[-idxr:] == b[-idxr:]:\n",
    "            idxr += 1\n",
    "        na, nb = len(a), len(b)\n",
    "        return a[:(idxl-1)] + '-' + a[(idxl-1):(na-idxr+1)] + '-vs-' + b[(idxl-1):(nb-idxr+1)] + '-' + b[(nb-idxr+1):]\n",
    "\n",
    "    # create lists of vtk files to be compared\n",
    "    linestyles_comp = ['k-', 'b-']\n",
    "\n",
    "    vtk_filenames_comp = []\n",
    "    history_outfile_comp = []\n",
    "    history_comp = []\n",
    "    for idx in range(2):\n",
    "        if datapaths_comp[idx][-4:] == '.tgz':\n",
    "            vtk_filenames_comp.append(sorted(\n",
    "                [x.path for x in tarfile.open(datapaths_comp[idx]).getmembers() if x.path[-4:] == '.vtk'] + [x.path[:-4] for x in tarfile.open(datapaths_comp[idx]).getmembers() if x.path[-8:] == '.vtk.pkl']\n",
    "            ))\n",
    "            history_outfile_comp.append(\n",
    "                ''.join([(x.path if 'history.pkl' in x.path else '') for x in tarfile.open(datapaths_comp[idx]).getmembers()])\n",
    "            )\n",
    "        else:\n",
    "            vtk_filenames_comp.append(\n",
    "                sorted(list(set(\n",
    "                    glob.glob(datapaths_comp[idx] + 'joined_vtk/*.vtk') + [x[:-4] for x in glob.glob(datapaths_comp[idx] + 'joined_vtk/*.vtk.pkl')]\n",
    "                )))\n",
    "            )\n",
    "            history_outfile_comp.append(datapaths_comp[idx] + 'history.pkl')\n",
    "\n",
    "        # precalculate histories if needed\n",
    "        # uncompressed datapath\n",
    "        if datapaths_comp[idx][-4:] != '.tgz':\n",
    "            if force_recalc or not os.path.exists(history_outfile_comp[idx]):\n",
    "                history_comp.append(precalc_history(vtk_filenames_comp[idx], out_dt=out_dt_vtk))\n",
    "                with open(history_outfile_comp[idx], 'wb') as f:\n",
    "                    pkl.dump(history_comp[idx], f)\n",
    "            else:\n",
    "                with open(history_outfile_comp[idx], 'rb') as f:\n",
    "                    history_comp.append(pkl.load(f))\n",
    "        # tar'red datapath\n",
    "        else:\n",
    "            extract = False\n",
    "            tar = tarfile.open(datapaths_comp[idx])\n",
    "            if force_recalc or history_outfile_comp[idx] not in [x.path for x in tar.getmembers()]:\n",
    "                # is we need to save *.pkl files and we are using tarfile, it needs to be extracted to be edited\n",
    "                print('Extracting tarfile %s to add history.pkl.. ' % datapaths_comp[idx], end='')\n",
    "                extract = True\n",
    "                tar.extractall(path = '/'.join(datapaths_comp[idx].split('/')[:-1]))\n",
    "                datapaths_comp[idx] = datapaths_comp[idx][:-4]+'/'\n",
    "                vtk_filenames_comp[idx] = [datapaths_comp[idx]+'/'.join(x.split('/')[1:]) for x in vtk_filenames_comp[idx]]\n",
    "                print('done.')\n",
    "\n",
    "                # calculate the history\n",
    "                history_comp.append(precalc_history(vtk_filenames_comp[idx], out_dt=out_dt_vtk))\n",
    "                # save the history\n",
    "                history_outfile = datapaths_comp[idx]+'history.pkl'\n",
    "                with open(history_outfile, 'wb') as f:\n",
    "                    pkl.dump(history_comp[idx], f)\n",
    "            else:\n",
    "                history_comp.append(pkl.load(tar.extractfile(history_outfile_comp[idx])))\n",
    "            tar.close()\n",
    "\n",
    "            # remove the tarfile if extracted\n",
    "            if extract:\n",
    "                os.remove(datapaths_comp[idx][:-1]+'.tgz')\n",
    "            \n",
    "                \n",
    "    force_recalc = False # recalc done\n",
    "\n",
    "    comp_name = diff_name(datapaths_comp[0], datapaths_comp[1])\n",
    "    outpath = comp_name + '_temp_comparison/'\n",
    "    outfile = comp_name + '_comparison.mp4'\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "\n",
    "    n_levels = 64\n",
    "    \n",
    "    def get_fileno (filename):\n",
    "        return int(filename.split('/')[-1].split('.')[1])\n",
    "\n",
    "    # WARNING: add_snapshot_FIFO below is NOT embarassingly parallelizable, but does significantly save memory\n",
    "    def comparison_frame (i_vtk, verbose=False, save=True, recalculate=False, history_comp=None, augment_kwargs=default_augment_kwargs, tarpaths=[None,None]):\n",
    "\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.gridspec as gridspec\n",
    "\n",
    "        from read_vtk import vtk\n",
    "\n",
    "        fileno = get_fileno(vtk_filenames_comp[0][i_vtk])\n",
    "        if fileno != get_fileno(vtk_filenames_comp[1][i_vtk]):\n",
    "            print('[comparison_frame] file lists not aligned. Aborting.')\n",
    "            return\n",
    "\n",
    "        vtk_time = fileno * out_dt_vtk # presumably we can trust this..\n",
    "\n",
    "        print('Processing vtk no %i, vtk_time = %.2e..' % (i_vtk, vtk_time), flush=True)\n",
    "\n",
    "        # see whether we can skip the file as already plotted\n",
    "        outfile = (outpath + 'comparison_%05i.png') % fileno\n",
    "        if save and not recalculate and os.path.exists(outfile):\n",
    "            print(' - file already plotted, skipping.', flush=True)\n",
    "            return\n",
    "\n",
    "        # initialize the plot\n",
    "        fig = plt.figure(figsize=(24,12))\n",
    "        gs = gridspec.GridSpec(4,8, width_ratios=[1,0.1,0.1,1,0.1,0.1,1,0.1])\n",
    "        plt.suptitle('Time = %.2f sec.' % (vtk_time * sim2phys['Time'],))\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        if verbose:\n",
    "            print(' - processing vtk data..', flush=True)\n",
    "\n",
    "        data_vtk = []\n",
    "        for idx in range(2):\n",
    "            # load, process, and plot the vtk file (mhd quantities)\n",
    "            try:\n",
    "                data_vtk = read_vtk_file(vtk_filenames_comp[idx][i_vtk], previous_data_vtk=None, out_dt=out_dt_vtk, augment_kwargs=augment_kwargs, tarpath=tarpaths[idx])\n",
    "            except Exception as e:\n",
    "                print('Could not reat vtk file ', vtk_filenames_comp[idx][i_vtk], 'continuiung..')\n",
    "                continue\n",
    "\n",
    "            # plot\n",
    "\n",
    "            plt.subplot(gs[idx,0])\n",
    "            plt.contourf(data_vtk['x1v'], data_vtk['x2v'], data_vtk['magnetization'], levels=n_levels)\n",
    "            plt.gca().axes.xaxis.set_ticklabels([])\n",
    "            plt.title('Magnetization (fl. frame)')\n",
    "            plt.ylabel('y [lt-sec]')\n",
    "            plt.colorbar(cax=plt.subplot(gs[idx,1]))\n",
    "\n",
    "            plt.subplot(gs[idx,3])\n",
    "            plt.contourf(data_vtk['x1v'], data_vtk['x2v'], data_vtk['flux_density'] * sim2phys['flux_density'], levels=n_levels)\n",
    "            plt.title('Syn. flux / dS [ erg $/$ (cm$^2$ s) $/$ cm$^2$ ]')\n",
    "            plt.gca().axes.xaxis.set_ticklabels([])\n",
    "            plt.ylabel('y [lt-sec]')\n",
    "            plt.colorbar(cax=plt.subplot(gs[idx,4]))\n",
    "\n",
    "            plt.subplot(gs[idx,6])\n",
    "            plt.contourf(data_vtk['x1v'], data_vtk['x2v'], data_vtk['rho'] * sim2phys['rho'], levels=n_levels)\n",
    "            plt.title('Density (fl. frame) [ g $/$ cm$^3$ ]')\n",
    "            plt.gca().axes.xaxis.set_ticklabels([])\n",
    "            plt.ylabel('y [lt-sec]')\n",
    "            plt.colorbar(cax=plt.subplot(gs[idx,7]))\n",
    "\n",
    "            # --------------------------------------\n",
    "\n",
    "            ax = plt.subplot(gs[2,0])\n",
    "            plt.plot(data_vtk['x1v'], data_vtk['Bcc_fluid_tot_vsZ'] * sim2phys['Bcc_fluid_tot_vsZ'], linestyles_comp[idx])\n",
    "            plt.gca().axes.xaxis.set_ticklabels([])\n",
    "            plt.title('Bfield (fl. frame) [ ( erg $/$ cm$^3 $)$^{1/2}$ ]')\n",
    "            ax.set_xlim(min(data_vtk['x1v']), max(data_vtk['x1v']))\n",
    "\n",
    "            ax = plt.subplot(gs[2,3])\n",
    "            plt.plot(data_vtk['x1v'], data_vtk['j_nu_vsZ'] * sim2phys['j_nu_vsZ'], linestyles_comp[idx])\n",
    "            plt.gca().axes.xaxis.set_ticklabels([])\n",
    "            plt.title('$j_{\\\\nu}(1.4$ PHz$)$ [ erg $/$ cm$^3$ ]')\n",
    "            ax.set_xlim(min(data_vtk['x1v']), max(data_vtk['x1v']))\n",
    "\n",
    "            ax = plt.subplot(gs[2,6])\n",
    "            plt.plot(data_vtk['x1v'], data_vtk['internal_energy_vsZ'] * sim2phys['internal_energy_vsZ'], linestyles_comp[idx])\n",
    "            plt.gca().axes.xaxis.set_ticklabels([])\n",
    "            plt.title('$U$ [ erg $/$ cm$^3$ ]')\n",
    "            ax.set_xlim(min(data_vtk['x1v']), max(data_vtk['x1v']))\n",
    "\n",
    "            ax = plt.subplot(gs[3,6])\n",
    "            plt.plot(\n",
    "                data_vtk['spectrum'][0] * sim2phys['spectrum'][0], \n",
    "                data_vtk['spectrum'][1] * sim2phys['spectrum'][1], linestyles_comp[idx])\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_yscale('log')\n",
    "            plt.xlabel('$\\\\nu$ [Hz]')\n",
    "            plt.ylabel('$F_{\\\\nu\\\\rm,syn}$ [ erg $/$ (s cm$^2$ Hz) $/$ cm$^2$ ]')\n",
    "            plt.axvline(augment_kwargs['nu_int_min'], color='k', ls=':')\n",
    "            plt.axvline(augment_kwargs['nu_int_max'], color='k', ls=':')\n",
    "\n",
    "            # set the plot range to vtk data\n",
    "            for j in range(2):\n",
    "                for i in range(3):\n",
    "                    ax = plt.subplot(gs[j,3*i])\n",
    "                    ax.set_xlim(min(data_vtk['x1v']), max(data_vtk['x1v']))\n",
    "                    ax.set_ylim(min(data_vtk['x2v']), max(data_vtk['x2v']))\n",
    "\n",
    "            # plot the history data\n",
    "            if type(history_comp[idx]) == dict:\n",
    "                ax_Udot = plt.subplot(gs[3,0])\n",
    "                ax_Fsyn = plt.subplot(gs[3,3])\n",
    "                mask = history_comp[idx]['times'] < vtk_time\n",
    "                ax_Udot.plot(\n",
    "                    history_comp[idx]['times'][mask] * sim2phys['Time'], \n",
    "                    history_comp[idx]['ddt_internal_energy'][mask] * sim2phys['ddt_internal_energy'], \n",
    "                    linestyles_comp[idx])\n",
    "                mask = history_comp[idx]['times'] == vtk_time\n",
    "                ax_Udot.scatter(\n",
    "                    history_comp[idx]['times'][mask] * sim2phys['Time'], \n",
    "                    history_comp[idx]['ddt_internal_energy'][mask] * sim2phys['ddt_internal_energy'],\n",
    "                    color=linestyles_comp[idx][0], s=8)\n",
    "                mask = history_comp[idx]['times'] > vtk_time\n",
    "                ax_Udot.plot(\n",
    "                    history_comp[idx]['times'][mask] * sim2phys['Time'], \n",
    "                    history_comp[idx]['ddt_internal_energy'][mask] * sim2phys['ddt_internal_energy'],\n",
    "                    (linestyles_comp[idx][0] + ':'))\n",
    "                ax_Udot.set_ylabel('$\\dot{U}_{\\\\rm avg}$ [erg $/$ (cm$^3$ s)]')\n",
    "                ax_Udot.set_xlabel('Time [sec]')\n",
    "\n",
    "                mask = history_comp[idx]['times'] < vtk_time\n",
    "                ax_Fsyn.plot(\n",
    "                    history_comp[idx]['times'][mask] * sim2phys['Time'],\n",
    "                    history_comp[idx]['flux_density'][mask] * sim2phys['flux_density'],\n",
    "                    linestyles_comp[idx])\n",
    "                mask = history_comp[idx]['times'] == vtk_time\n",
    "                ax_Fsyn.scatter(\n",
    "                    history_comp[idx]['times'][mask] * sim2phys['Time'],\n",
    "                    history_comp[idx]['flux_density'][mask] * sim2phys['flux_density'],\n",
    "                    color=linestyles_comp[idx][0], s=8)\n",
    "                mask = history_comp[idx]['times'] > vtk_time\n",
    "                ax_Fsyn.plot(\n",
    "                    history_comp[idx]['times'][mask] * sim2phys['Time'],\n",
    "                    history_comp[idx]['flux_density'][mask] * sim2phys['flux_density'],\n",
    "                    (linestyles_comp[idx][0]+':'))\n",
    "                plt.ylabel('Avg. syn. flux / dS [${\\\\rm erg}/({\\\\rm cm}^2{\\\\rm s}) / {\\\\rm cm}^2$]')\n",
    "\n",
    "                # set up limits\n",
    "                if npmax(history_comp[0]['times'] * sim2phys['Time']) > 4.0 and npmax(history_comp[1]['times'] * sim2phys['Time']) > 4.0:\n",
    "                    mask = [\n",
    "                        (history_comp[0]['times'] * sim2phys['Time'] > 4.0),\n",
    "                        (history_comp[1]['times'] * sim2phys['Time'] > 4.0)\n",
    "                    ]\n",
    "                else:\n",
    "                    mask = [\n",
    "                        (history_comp[0]['times'] * sim2phys['Time'] > 0.0),\n",
    "                        (history_comp[1]['times'] * sim2phys['Time'] > 0.0)\n",
    "                    ]\n",
    "                ax_Fsyn.set_ylim(\n",
    "                    None,\n",
    "                    npmax([\n",
    "                        npmax(history_comp[0]['flux_density'][mask[0]]) * sim2phys['flux_density'],\n",
    "                        npmax(history_comp[1]['flux_density'][mask[1]]) * sim2phys['flux_density']\n",
    "                    ])\n",
    "                )\n",
    "\n",
    "            # clean up\n",
    "            del data_vtk\n",
    "\n",
    "        if verbose:\n",
    "            print('     done.', flush=True)\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        if save:\n",
    "            plt.savefig(outfile, format='png', dpi=300, facecolor='w')\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        print(' - frame done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if processing_type == 'comparison':\n",
    "    # is we need to save *.pkl files and we are using tarfile, it needs to be extracted to be edited\n",
    "    tarpaths = [None, None]\n",
    "    extract = [False, False]\n",
    "    for idx in range(2):\n",
    "        if '.tgz' in datapaths_comp[idx]:\n",
    "            if os.path.isdir('.'.join(datapaths_comp[idx].split('.')[:-1])):\n",
    "                # tar already extracted\n",
    "                datapaths_comp[idx] = os.path.isdir('.'.join(datapaths_comp[idx].split('.')[:-1]))\n",
    "                tarpaths[idx] = None\n",
    "            elif convert_vtk:\n",
    "                # do we need to extract to convert vtk to pkl?\n",
    "                contents = [(1 if x.path[:-4] == '.vtk' else 0) for x in tarfile.open(datapaths_comp[idx]).getmembers()]\n",
    "                extract[idx] = (np.sum(contents) > 0)\n",
    "                if extract[idx]:\n",
    "                    print('Extracting tarfile %s.. ' % datapaths_comp[idx], end='')\n",
    "                    tar = tarfile.open(datapaths_comp[idx])\n",
    "                    tar.extractall(path = '.'.join(datapaths_comp[idx].split['.'][:-1]))\n",
    "                    datapaths_comp[idx] = datapaths_comp[idx][:-4]+'/'\n",
    "                    vtk_filenames_comp[idx] = [datapaths_comp[idx]+'/'.join(x.split('/')[1:]) for x in vtk_filenames_comp[idx]]\n",
    "                    tar.close()\n",
    "                    tarpaths[idx] = None\n",
    "                    print('done.')\n",
    "                else:\n",
    "                    # otherwise keep the tar and read files directly from there\n",
    "                    tarpaths[idx] = datapaths_comp[idx]\n",
    "            else:\n",
    "                tarpaths[idx] = datapaths_comp[idx]\n",
    "    \n",
    "    # fill in the gaps in datasets\n",
    "    for idx in [0,1]:\n",
    "        vtk_filenames_comp[idx] = sum([[elem1,]*(get_fileno(elem2)-get_fileno(elem1)) for elem1,elem2 in zip(vtk_filenames_comp[idx][:-1], vtk_filenames_comp[idx][1:])], []) + [vtk_filenames_comp[idx][-1],]\n",
    "    lendiff = len(vtk_filenames_comp[1]) - len(vtk_filenames_comp[0])\n",
    "    if lendiff > 0:\n",
    "        vtk_filenames_comp[0] += [vtk_filenames_comp[0][-1],]*lendiff\n",
    "    elif lendiff < 0:\n",
    "        vtk_filenames_comp[1] += [vtk_filenames_comp[1][-1],]*(-lendiff)\n",
    "    print('Dataset alignment:')\n",
    "    for i in range(len(vtk_filenames_comp[0])):\n",
    "        print('%i -- %i' % (get_fileno(vtk_filenames_comp[0][i]),get_fileno(vtk_filenames_comp[1][i])))\n",
    "    print(' --- ')\n",
    "    \n",
    "    # now, parallelize the frame generation\n",
    "    from pathos.pools import ProcessPool # an alternative to Python's multiprocessing\n",
    "    chunks = np.array_split(range(len(vtk_filenames_comp[0])), nproc)\n",
    "    def worker (ichunk):\n",
    "        indices = chunks[ichunk]\n",
    "        data_vtk = None\n",
    "        for i in indices:\n",
    "            data_vtk = comparison_frame(i_vtk=i, recalculate=force_recalc, history_comp=history_comp, augment_kwargs=default_augment_kwargs, verbose=False, tarpaths=tarpaths)\n",
    "    with ProcessPool(nproc) as pool:\n",
    "        _ = pool.map(worker, list(range(nproc)))\n",
    "    # tar when done and clean up\n",
    "    for idx in range(2):\n",
    "        if extract[idx] or (tar_when_done and '.tgz' not in datapaths_comp[idx]):\n",
    "            print('Archiving into tarfile %s.. ' % datapaths_comp[idx], end='')\n",
    "            workdir = os.getcwd()\n",
    "            tar_location = '/'.join(datapaths_comp[idx].split('/')[:-2])\n",
    "            if tar_location != '':\n",
    "                os.chdir(tar_location)\n",
    "            try:\n",
    "                os.system('cp %s %s' % (datapaths_comp[idx].split('/')[-2]+'/history.pkl', 'history_%s.pkl' % (datapaths_comp[idx].split('/')[-2].split('_')[-1],)))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            os.system('tar -cvzf %s %s' % (datapaths_comp[idx].split('/')[-2] + '.tgz',datapaths_comp[idx].split('/')[-2]))\n",
    "            if os.path.isfile(datapaths_comp[idx][:-1] + '.tgz'):\n",
    "                rmtree(datapaths_comp[idx])\n",
    "                datapaths_comp[idx] = datapaths_comp[idx][:-1] + '.tgz'\n",
    "                tarpaths[idx] = datapaths_comp[idx]\n",
    "            os.chdir(workdir)\n",
    "            print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if processing_type == 'comparison' and in_script:\n",
    "    # render the movie\n",
    "    try:\n",
    "        print(\"Rendering the movie..\", flush=True)\n",
    "        command = (\"ffmpeg -threads %i -y -r 20 -f image2 -i \\\"%scomparison_%%*.png\\\" -f mp4 -q:v 0 -vcodec mpeg4 -r 20 %s\" % (nproc, outpath, outfile))\n",
    "        print(command, flush=True)\n",
    "        os.system(command)\n",
    "        if os.path.isfile(outfile):\n",
    "            os.system('rm -r ' + outpath)\n",
    "        else:\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        print('Error while rendering movie:\\n%s\\n -- please try to manually convert the .png files generated in %s.' % (e, outpath), flush=True)\n",
    "    print(\"COMPARISON PROCESSING DONE.\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if processing_type == 'comparison' and not in_script:\n",
    "    comparison_frame(80, history_comp=history_comp, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Experiment regarding long-term Fsyn enhancement\n",
    " (see Pjanka et al. 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if processing_type == 'expLongFsyn':\n",
    "    \n",
    "    def calc_alternative_augmentations (default_dict, new_bcc_dict, \n",
    "                      nu_res=128, nu_min=1., nu_max=1.1*nu_int_max,\n",
    "                      nu_int_min=nu_int_min, nu_int_max=nu_int_max,\n",
    "                      R_selection=R_choice, nu_selection=nu_choice,\n",
    "                      filling_factor=1.0,\n",
    "                    calc_spectrum=False):\n",
    "        '''Uses the default-augmented file and appends Bfield and Fsyn data for alternative scenarios:\n",
    "         - with Bfield from a separate non-corrugated (1D) run\n",
    "         - with Bfield from the corrugated (2D) run but vertically-averaged into a 1D structure\n",
    "         - with the above scaled so that the average is the same as the 1D run, leaving only the horizontal structure (but not the overall bfield enhancement)\n",
    "         - with Bfield from the corrugated (2D) run scaled down so that the average matches the 1D case, leaving only the 2D structure (but not the overall bfield enhancement)'''\n",
    "        \n",
    "        data_vtk = default_dict\n",
    "    \n",
    "        # box dimensions\n",
    "        xrange = (data_vtk['x1v'][1] - data_vtk['x1v'][0]) * len(data_vtk['x1v'])\n",
    "        yrange = (data_vtk['x2v'][1] - data_vtk['x2v'][0]) * len(data_vtk['x2v'])\n",
    "        for f in [1,2]:\n",
    "            new_bcc_dict[f'x{f}v'] = data_vtk[f'x{f}v']\n",
    "\n",
    "        # SR quantities\n",
    "        gam = data_vtk['gamma']\n",
    "\n",
    "        # total Bcc in observer frame\n",
    "        new_bcc_dict['Bcc_tot'] = tf_deconvert(norm_vec_l2(new_bcc_dict['Bcc1'], new_bcc_dict['Bcc2'], new_bcc_dict['Bcc3']))\n",
    "\n",
    "        # Bcc in the fluid frame\n",
    "        Bfl0 = BccFl0(\n",
    "            gam,\n",
    "            data_vtk['vel1'],data_vtk['vel2'],data_vtk['vel3'],\n",
    "            new_bcc_dict['Bcc1'],new_bcc_dict['Bcc2'],new_bcc_dict['Bcc3'],\n",
    "        )\n",
    "        Bfl1 = BccFli(\n",
    "            gam, \n",
    "            data_vtk['vel1'], new_bcc_dict['Bcc1'],\n",
    "            Bfl0\n",
    "        )\n",
    "        Bfl2 = BccFli(\n",
    "            gam, \n",
    "            data_vtk['vel2'], new_bcc_dict['Bcc2'],\n",
    "            Bfl0\n",
    "        )\n",
    "        Bfl3 = BccFli(\n",
    "            gam, \n",
    "            data_vtk['vel3'], new_bcc_dict['Bcc3'],\n",
    "            Bfl0\n",
    "        )\n",
    "        Bcc_fluid_tot_sqr = sqr_vec_l2(Bfl1, Bfl2, Bfl3)\n",
    "        Bcc_fluid_tot = sqrt(Bcc_fluid_tot_sqr)\n",
    "        new_bcc_dict['Bcc_fluid_0'] = tf_deconvert(Bfl0)\n",
    "        new_bcc_dict['Bcc_fluid_1'] = tf_deconvert(Bfl1)\n",
    "        new_bcc_dict['Bcc_fluid_2'] = tf_deconvert(Bfl2)\n",
    "        new_bcc_dict['Bcc_fluid_3'] = tf_deconvert(Bfl3)\n",
    "        new_bcc_dict['Bcc_fluid_tot'] = tf_deconvert(Bcc_fluid_tot)\n",
    "        do_vertical_avg(new_bcc_dict, 'Bcc_fluid_tot')\n",
    "\n",
    "        # plasma parameters\n",
    "        new_bcc_dict['plasma_beta'] = tf_deconvert(plasma_beta(data_vtk['press'], Bcc_fluid_tot_sqr))\n",
    "        new_bcc_dict['magnetization'] = tf_deconvert(magnetization(Bcc_fluid_tot_sqr, data_vtk['rho']))\n",
    "\n",
    "        # internal energy in the fluid frame\n",
    "        # see Beckwith & Stone (2011), https://github.com/PrincetonUniversity/athena/wiki/Special-Relativity\n",
    "        enth = data_vtk['enthalpy']\n",
    "        new_bcc_dict['internal_energy'] = tf_deconvert(internal_energy(data_vtk['rho'], enth, gam, data_vtk['press'], Bcc_fluid_tot_sqr)) # warning!: includes rest mass\n",
    "        do_vertical_avg(new_bcc_dict, 'internal_energy')\n",
    "\n",
    "        del Bcc_fluid_tot_sqr, enth\n",
    "\n",
    "        # synchrotron emission diagnostics\n",
    "        jnu = j_nu(nu2nu_fl(nu_selection,doppler_factor), Bcc_fluid_tot)\n",
    "        new_bcc_dict['j_nu'] = tf_deconvert(jnu)\n",
    "        do_vertical_avg(new_bcc_dict, 'j_nu')\n",
    "        alphanu = alpha_nu(nu2nu_fl(nu_selection,doppler_factor), Bcc_fluid_tot)\n",
    "        new_bcc_dict['alpha_nu'] = tf_deconvert(alphanu)\n",
    "        do_vertical_avg(new_bcc_dict, 'alpha_nu')\n",
    "        janu = j_over_alpha_nu(nu2nu_fl(nu_selection,doppler_factor), Bcc_fluid_tot)\n",
    "        new_bcc_dict['j_over_alpha_nu'] = tf_deconvert(janu)\n",
    "        do_vertical_avg(new_bcc_dict, 'j_over_alpha_nu')\n",
    "        flux_tot = flux_total_per_dS(B=Bcc_fluid_tot, R=R_selection, nu_min=nu_int_min, nu_max=nu_int_max)\n",
    "        new_bcc_dict['flux_density'] = tf_deconvert(flux_tot)\n",
    "        do_vertical_avg(new_bcc_dict, 'flux_density')\n",
    "\n",
    "        if calc_spectrum:\n",
    "            nu_min, nu_max = tf_convert(nu_min, nu_max)\n",
    "            freqs = logspace(log10(nu_min), log10(nu_max), nu_res)\n",
    "\n",
    "            dS = (data_vtk['x1v'][1] - data_vtk['x1v'][0]) * (data_vtk['x2v'][1] - data_vtk['x2v'][0])\n",
    "            if not low_memory:\n",
    "                nu_grid, B_grid = meshgrid(freqs, Bcc_fluid_tot, indexing='ij')\n",
    "                new_bcc_dict['spectrum'] = [\n",
    "                    tf_deconvert(freqs),\n",
    "                    tf_deconvert(nansum(\n",
    "                        flux_nu_per_dS(\n",
    "                            nu=nu_grid, \n",
    "                            B=B_grid, \n",
    "                            gamma=combined_gamma,\n",
    "                            doppler_factor=doppler_factor,\n",
    "                            R=R_selection, \n",
    "                            filling_factor=filling_factor\n",
    "                        )*dS, axis=-1) / (xrange*yrange))\n",
    "                ]\n",
    "            else:\n",
    "                new_bcc_dict['spectrum'] = [[],[]]\n",
    "                for nu in tf_deconvert(freqs):\n",
    "                    new_bcc_dict['spectrum'][0].append(nu)\n",
    "                    new_bcc_dict['spectrum'][1].append(tf_deconvert(\n",
    "                        nansum(\n",
    "                            flux_nu_per_dS(\n",
    "                                nu=nu,\n",
    "                                B=Bcc_fluid_tot,\n",
    "                                gamma=combined_gamma,\n",
    "                                doppler_factor=doppler_factor,\n",
    "                                R=R_selection,\n",
    "                                filling_factor=filling_factor\n",
    "                            )\n",
    "                        ) * dS / (xrange*yrange)\n",
    "                    ))\n",
    "                new_bcc_dict['spectrum'] = [np.array(new_bcc_dict['spectrum'][0]), np.array(new_bcc_dict['spectrum'][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if processing_type == 'expLongFsyn':\n",
    "    \n",
    "    def augment_Bfield_alternatives (augmented_2d_pkl, augmented_1d_pkl, append=True):\n",
    "        '''Uses the default-augmented file and appends Bfield and Fsyn data for alternative scenarios:\n",
    "         - with Bfield from a separate non-corrugated (1D) run\n",
    "         - with Bfield from the corrugated (2D) run but vertically-averaged into a 1D structure\n",
    "         - with the above scaled so that the average is the same as the 1D run, leaving only the horizontal structure (but not the overall bfield enhancement)\n",
    "         - with Bfield from the corrugated (2D) run scaled down so that the average matches the 1D case, leaving only the 2D structure (but not the overall bfield enhancement)'''\n",
    "        # First, extract both files\n",
    "        with open(augmented_1d_pkl, 'rb') as f:\n",
    "            data_1d, _ = pkl.load(f)\n",
    "        with open(augmented_2d_pkl, 'rb') as f:\n",
    "            data_2d, augment_kwargs = pkl.load(f)\n",
    "        jres = data_2d['Bcc1'].shape[0]\n",
    "        # Process the alternatives\n",
    "        data_2d['alternatives'] = {}\n",
    "        # Test: create identical flux\n",
    "        if True:\n",
    "            alternative = 'identity'\n",
    "            data_2d['alternatives'][alternative] = {}\n",
    "            for f in [1,2,3]:\n",
    "                data_2d['alternatives'][alternative][f'Bcc{f}'] = data_2d[f'Bcc{f}']\n",
    "            calc_alternative_augmentations(\n",
    "                data_2d, data_2d['alternatives'][alternative],\n",
    "                **augment_kwargs\n",
    "            )\n",
    "        # - with Bfield from a separate non-corrugated (1D) run\n",
    "        alternative = 'B1d_sep'\n",
    "        data_2d['alternatives'][alternative] = {}\n",
    "        for f in [1,2,3]:\n",
    "            data_2d['alternatives'][alternative][f'Bcc{f}'] = np.repeat(data_1d[f'Bcc{f}'][:1], jres, axis=0)\n",
    "        calc_alternative_augmentations(\n",
    "            data_2d, data_2d['alternatives'][alternative],\n",
    "            **augment_kwargs\n",
    "        )\n",
    "        # - with Bfield from the corrugated (2D) run but vertically-averaged into a 1D structure\n",
    "        alternative = 'B1d_avg'\n",
    "        data_2d['alternatives'][alternative] = {}\n",
    "        for f in [1,2,3]:\n",
    "            data_2d['alternatives'][alternative][f'Bcc{f}'] = np.repeat(np.mean(data_2d[f'Bcc{f}'], axis=0, keepdims=True), jres, axis=0)\n",
    "        calc_alternative_augmentations(\n",
    "            data_2d, data_2d['alternatives'][alternative],\n",
    "            **augment_kwargs\n",
    "        )\n",
    "        # - with the above scaled so that the average is the same as the 1D run, leaving only the horizontal structure (but not the overall bfield enhancement)\n",
    "        alternative = 'B1d_avgScaled'\n",
    "        data_2d['alternatives'][alternative] = {}\n",
    "        for f in [1,2,3]:\n",
    "            norm = np.sum(data_2d['alternatives']['B1d_avg'][f'Bcc{f}'][0])\n",
    "            if norm != 0.:\n",
    "                data_2d['alternatives'][alternative][f'Bcc{f}'] = data_2d['alternatives']['B1d_avg'][f'Bcc{f}'] * np.sum(data_1d[f'Bcc{f}'][0]) / norm\n",
    "            else:\n",
    "                data_2d['alternatives'][alternative][f'Bcc{f}'] = data_2d['alternatives']['B1d_avg'][f'Bcc{f}']\n",
    "            del norm\n",
    "        calc_alternative_augmentations(\n",
    "            data_2d, data_2d['alternatives'][alternative],\n",
    "            **augment_kwargs\n",
    "        )\n",
    "        # - with Bfield from the corrugated (2D) run scaled down so that the average matches the 1D case, leaving only the 2D structure (but not the overall bfield enhancement)\n",
    "        alternative = 'B2d_scaled'\n",
    "        data_2d['alternatives'][alternative] = {}\n",
    "        for f in [1,2,3]:\n",
    "            norm = np.sum(data_2d[f'Bcc{f}'])\n",
    "            if norm != 0.:\n",
    "                data_2d['alternatives'][alternative][f'Bcc{f}'] = data_2d[f'Bcc{f}'] * np.sum(data_1d[f'Bcc{f}'][:1])*jres / np.sum(data_2d[f'Bcc{f}'])\n",
    "            else:\n",
    "                data_2d['alternatives'][alternative][f'Bcc{f}'] = data_2d[f'Bcc{f}']\n",
    "            del norm\n",
    "        calc_alternative_augmentations(\n",
    "            data_2d, data_2d['alternatives'][alternative],\n",
    "            **augment_kwargs\n",
    "        )\n",
    "        # Save the alternative-appended result to the 2d file\n",
    "        if append:\n",
    "            with open(augmented_2d_pkl, 'wb') as f:\n",
    "                pkl.dump((data_2d, augment_kwargs), f)\n",
    "        del data_1d\n",
    "        return data_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processing_type == 'expLongFsyn':\n",
    "    \n",
    "    def precalc_alternate_history (history_pkl, vtk_pkl_filenames_1d2d_pairs, append=True):\n",
    "        '''Note: requires the archive containing *.vtk.pkl files to be untarred (to be able to append the pkl files), extract first if needed.'''\n",
    "        # load the original history\n",
    "        with open(history_pkl, 'rb') as f:\n",
    "            history = pkl.load(f)\n",
    "        # prepare to append\n",
    "        quantities = ('internal_energy', 'flux_density')\n",
    "        alternatives = ('B1d_sep', 'B1d_avg', 'B1d_avgScaled', 'B2d_scaled', 'identity')\n",
    "        history['alternatives'] = {a:{q:[] for q in quantities} for a in alternatives}\n",
    "        # processing file-by-file\n",
    "        for pkl_1d, pkl_2d in tqdm(vtk_pkl_filenames_1d2d_pairs):\n",
    "            # augment the files\n",
    "            try:\n",
    "                data_2d = augment_Bfield_alternatives (pkl_2d, pkl_1d, append=True)\n",
    "            except Exception as e:\n",
    "                print(f'[precalc_alternate_history] Could not read vtk.pkl files:\\n  {pkl_1d, pkl_2d}\\n  error msg: {e}')\n",
    "                print(' - the file will be ignored.')\n",
    "                continue\n",
    "            # calculate history variables\n",
    "            data_vtk = data_2d\n",
    "            xrange = (data_vtk['x1v'][1] - data_vtk['x1v'][0]) * len(data_vtk['x1v'])\n",
    "            dl = (data_vtk['x1v'][1] - data_vtk['x1v'][0])\n",
    "            for alternative in alternatives:\n",
    "                history_branch = history['alternatives'][alternative]\n",
    "                data_branch = data_2d['alternatives'][alternative]\n",
    "                history_branch['internal_energy'].append(\n",
    "                    get_cgs_value(np.sum(data_branch['internal_energy_vsZ']*dl))/xrange\n",
    "                )\n",
    "                history_branch['flux_density'].append(\n",
    "                    get_cgs_value(np.sum(data_branch['flux_density_vsZ']*dl))/xrange\n",
    "                )\n",
    "        for alternative in alternatives:\n",
    "            history_branch = history['alternatives'][alternative]\n",
    "            # convert to numpy\n",
    "            for quantity in quantities:\n",
    "                history_branch[quantity] = np.array(history_branch[quantity])\n",
    "            # calculate derivatives\n",
    "            history_branch['ddt_internal_energy'] = (history_branch['internal_energy'][1:] - history_branch['internal_energy'][:-1]) / (history['times'][1:] - history['times'][:-1])\n",
    "            history_branch['ddt_internal_energy'] = np.insert(history_branch['ddt_internal_energy'], 0, np.nan)\n",
    "        # Save the alternative-appended result to the 2d file\n",
    "        if append:\n",
    "            with open(history_pkl, 'wb') as f:\n",
    "                pkl.dump(history, f)\n",
    "        else:\n",
    "            return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processing_type == 'expLongFsyn':\n",
    "    \n",
    "    # Preprocessing and history augmentation\n",
    "    \n",
    "    def diff_name(a,b):\n",
    "        \"\"\"Returns a comparison-style name based on two strings\"\"\"\n",
    "        # don't edit the original paths\n",
    "        a,b = copy(a), copy(b)\n",
    "        # strip the .tgz, if needed\n",
    "        if a[-4:] == '.tgz': a = a[:-4]\n",
    "        if b[-4:] == '.tgz': b = b[:-4]\n",
    "        # strip the trailing /\n",
    "        if a[-1] == '/': a = a[:-1]\n",
    "        if b[-1] == '/': b = b[:-1]\n",
    "        # first, read from the front\n",
    "        idxl = 0\n",
    "        while a[:idxl] == b[:idxl]:\n",
    "            idxl += 1\n",
    "        # then, read from the back\n",
    "        idxr = 1\n",
    "        while a[-idxr:] == b[-idxr:]:\n",
    "            idxr += 1\n",
    "        na, nb = len(a), len(b)\n",
    "        return a[:(idxl-1)] + '-' + a[(idxl-1):(na-idxr+1)] + '-vs-' + b[(idxl-1):(nb-idxr+1)] + '-' + b[(nb-idxr+1):]\n",
    "    \n",
    "    def get_fileno (filename):\n",
    "        return int(filename.split('/')[-1].split('.')[1])\n",
    "\n",
    "    linestyles_comp = ['k-', 'b-']\n",
    "\n",
    "    # create lists of vtk files to be compared\n",
    "    vtk_filenames_comp = []\n",
    "    history_outfile_comp = []\n",
    "    history_comp = []\n",
    "    for idx in range(2):\n",
    "        # Extract the dataset if needed\n",
    "        if datapaths_comp[idx][-4:] == '.tgz':\n",
    "            os.system(f'cd $(dirname {datapaths_comp[idx]}) && tar --use-compress-program=pigz -xf $(basename {datapaths_comp[idx]}')\n",
    "        # list the files needed\n",
    "        vtk_filenames_comp.append(\n",
    "            sorted(list(set(\n",
    "                glob.glob(datapaths_comp[idx] + 'joined_vtk/*.vtk.pkl')\n",
    "            )))\n",
    "        )\n",
    "        history_outfile_comp.append(datapaths_comp[idx] + 'history.pkl')\n",
    "    \n",
    "    # fill in the gaps in datasets\n",
    "    for idx in [0,1]:\n",
    "        vtk_filenames_comp[idx] = sum([[elem1,]*(get_fileno(elem2)-get_fileno(elem1)) for elem1,elem2 in zip(vtk_filenames_comp[idx][:-1], vtk_filenames_comp[idx][1:])], []) + [vtk_filenames_comp[idx][-1],]\n",
    "    lendiff = len(vtk_filenames_comp[1]) - len(vtk_filenames_comp[0])\n",
    "    if lendiff > 0:\n",
    "        vtk_filenames_comp[0] += [vtk_filenames_comp[0][-1],]*lendiff\n",
    "    elif lendiff < 0:\n",
    "        vtk_filenames_comp[1] += [vtk_filenames_comp[1][-1],]*(-lendiff)\n",
    "    print('Dataset alignment:')\n",
    "    for i in range(len(vtk_filenames_comp[0])):\n",
    "        print('%i -- %i' % (get_fileno(vtk_filenames_comp[0][i]),get_fileno(vtk_filenames_comp[1][i])))\n",
    "    print(' --- ')\n",
    "                      \n",
    "    # Calculate the alternative histories\n",
    "    precalc_alternate_history(history_outfile_comp[1], zip(*vtk_filenames_comp), append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processing_type == 'expLongFsyn':\n",
    "    \n",
    "    def expLongFsyn_frame (i_vtk, verbose=False, save=True, recalculate=False, history_comp=None, augment_kwargs=default_augment_kwargs, tarpaths=[None,None]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Magnetic field curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processing_type == 'curvature':\n",
    "    \n",
    "    # check if we are reading from a folder or from a tarfile\n",
    "    if ('.tgz' in datapath) and os.path.isdir(datapath[:-4]):\n",
    "        print('Tarfile %s has already been extracted, using the extracted version.' % datapath)\n",
    "        datapath = datapath[:-4]+'/'\n",
    "    using_tarfile = ('.tgz' in datapath)\n",
    "\n",
    "    if not using_tarfile:\n",
    "        vtk_filenames = sorted(list(set(\n",
    "            glob.glob(datapath + 'joined_vtk/*.vtk') + [x[:-4] for x in glob.glob(datapath + 'joined_vtk/*.vtk.pkl')]\n",
    "        )))\n",
    "    else:\n",
    "        tar = tarfile.open(datapath)\n",
    "        vtk_filenames = sorted(\n",
    "            [x.path for x in tar.getmembers() if (x.path[-4:] == '.vtk')] + [x.path[:-4] for x in tar.getmembers() if (x.path[-8:] == '.vtk.pkl')]\n",
    "        )\n",
    "        tar.close()\n",
    "    print(f'Found {len(vtk_filenames)} vtk files.', flush=True)\n",
    "\n",
    "    outpath = f'./temp_curvature_{datapath.split(\"/\")[-2].split(\".\")[0]}/'\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "\n",
    "    # WARNING: add_snapshot_FIFO below is NOT embarassingly parallelizable, but does significantly save memory\n",
    "    def curvature_frame (i_vtk, verbose=False, save=True, recalculate=False, tarpath=None, force_recalc=False):\n",
    "\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.gridspec as gridspec\n",
    "        import pickle as pkl\n",
    "\n",
    "        fileno = int(vtk_filenames[i_vtk].split('/')[-1].split('.')[1])\n",
    "        vtk_time = fileno * out_dt_vtk # presumably we can trust this..\n",
    "\n",
    "        print('Processing vtk no %i, vtk_time = %.2e..' % (i_vtk, vtk_time), flush=True)\n",
    "\n",
    "        # see whether we can skip the file as already plotted\n",
    "        outfile = (outpath + 'curvature_%05i.png') % fileno\n",
    "        if save and not recalculate and os.path.exists(outfile):\n",
    "            print(' - file already plotted, skipping.', flush=True)\n",
    "            return\n",
    "        \n",
    "        # read the file\n",
    "        data = read_vtk_file(vtk_filenames[i_vtk], previous_data_vtk=None, out_dt=out_dt_vtk, tarpath=tarpath)\n",
    "        \n",
    "        # augment if needed\n",
    "        if 'curvature' not in data.keys() or force_recalc:\n",
    "            data['curvature'] = bfield_curvature(data)\n",
    "            if os.path.isfile(f'{vtk_filenames[i_vtk]}.pkl') and not tarpath:\n",
    "                with open(vtk_filenames[i_vtk], 'wb') as f:\n",
    "                    pkl.dump(data, f)\n",
    "        \n",
    "        # initialize the plot\n",
    "        fig = plt.figure(figsize=(24,12))\n",
    "        gs = gridspec.GridSpec(4,1)\n",
    "        plt.suptitle('Time = %.2f sec.' % (vtk_time * sim2phys['Time'],))\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        \n",
    "        # curvature contour plot\n",
    "        plt.subplot(gs[0,0])\n",
    "        \n",
    "        curvature_to_plot = np.where(\n",
    "            data['curvature'] > np.exp(-15),\n",
    "            np.log(data['curvature']), -15\n",
    "        )\n",
    "\n",
    "        plt.contourf(\n",
    "            data['x1v'][1:-1], data['x2v'][1:-1],\n",
    "            curvature_to_plot,\n",
    "            128,\n",
    "            cmap='magma')\n",
    "        #plt.gca().set_aspect(1)\n",
    "        plt.colorbar(label='Log Curvature')\n",
    "        \n",
    "        # curvature histogram\n",
    "        plt.subplot(gs[1,0])\n",
    "        \n",
    "        bins = np.linspace(np.min(curvature_to_plot),np.max(curvature_to_plot), 128)\n",
    "        hist = np.transpose([np.histogram(row, bins=bins)[0] for row in np.transpose(curvature_to_plot)])\n",
    "\n",
    "        plt.contourf(data['x1v'][1:-1], 0.5*(bins[1:]+bins[:-1]), np.log(hist), 128, cmap='magma')\n",
    "        plt.colorbar(label='log N')\n",
    "        plt.ylabel('Log N')\n",
    "        plt.xlabel('x[sim.u.]')\n",
    "        plt.ylabel('Log Bfield curvature')\n",
    "        plt.title('Bfield curvature histogram')\n",
    "\n",
    "        plt.plot(\n",
    "            data['x1v'][1:-1],\n",
    "            np.mean(curvature_to_plot, axis=0),\n",
    "            color='green',\n",
    "            label='mean'\n",
    "        )\n",
    "        plt.legend()\n",
    "        \n",
    "        del bins, hist\n",
    "        \n",
    "        # Bfield histograms\n",
    "        plt.subplot(gs[2,0])\n",
    "        \n",
    "        bins = np.linspace(0.,np.max(np.abs(data['Bcc1'])), 128)\n",
    "        hist = np.transpose([np.histogram(np.abs(row), bins=bins)[0] for row in np.transpose(data['Bcc1'])])\n",
    "\n",
    "        plt.contourf(data['x1v'], 0.5*(bins[1:]+bins[:-1]), np.log(hist), 128, cmap='magma')\n",
    "        plt.colorbar(label='log N')\n",
    "        plt.xlabel('x[sim.u.]')\n",
    "        plt.ylabel('Perpendicular Bfield')\n",
    "        plt.title('Perpendicular Bfield histogram')\n",
    "\n",
    "        plt.plot(\n",
    "            data['x1v'],\n",
    "            np.mean(np.abs(data['Bcc1']), axis=0),\n",
    "            label='mean', color='green'\n",
    "        )\n",
    "        plt.legend()\n",
    "        \n",
    "        del bins, hist\n",
    "        \n",
    "        plt.subplot(gs[3,0])\n",
    "        \n",
    "        bins = np.linspace(0.,np.max(np.abs(data['Bcc2'])), 128)\n",
    "        hist = np.transpose([np.histogram(np.abs(row), bins=bins)[0] for row in np.transpose(data['Bcc2'])])\n",
    "\n",
    "        plt.contourf(data['x1v'], 0.5*(bins[1:]+bins[:-1]), np.log(hist), 128, cmap='magma')\n",
    "        plt.colorbar(label='log N')\n",
    "        plt.xlabel('x[sim.u.]')\n",
    "        plt.ylabel('Parallel Bfield')\n",
    "        plt.title('Parallel Bfield histogram')\n",
    "\n",
    "        plt.plot(\n",
    "            data['x1v'],\n",
    "            np.mean(np.abs(data['Bcc2']), axis=0),\n",
    "            label='mean', color='green'\n",
    "        )\n",
    "        plt.legend()\n",
    "\n",
    "        # clean up\n",
    "        del data, bins, hist, curvature_to_plot\n",
    "\n",
    "        if verbose:\n",
    "            print('     done.', flush=True)\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        if save:\n",
    "            plt.savefig(outfile, format='png', dpi=300, facecolor='w')\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        print(' - frame done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processing_type == 'curvature' and in_script:\n",
    "    tarpath = None\n",
    "    if '.tgz' in datapath:\n",
    "        tarpath = datapath\n",
    "    # now, parallelize the frame generation\n",
    "    from pathos.pools import ProcessPool # an alternative to Python's multiprocessing\n",
    "    chunks = np.array_split(range(len(vtk_filenames)), nproc)\n",
    "    def worker (ichunk):\n",
    "        indices = chunks[ichunk]\n",
    "        for i in indices:\n",
    "            data_vtk = curvature_frame(i_vtk=i, recalculate=force_recalc, verbose=False, tarpath=tarpath)\n",
    "    with ProcessPool(nproc) as pool:\n",
    "        _ = pool.map(worker, list(range(nproc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processing_type == 'curvature' and in_script:\n",
    "    # render the movie\n",
    "    try:\n",
    "        print(\"Rendering the movie..\", flush=True)\n",
    "        command = (\"ffmpeg -threads %i -y -r 20 -f image2 -i \\\"%scurvature_%%*.png\\\" -f mp4 -q:v 0 -vcodec mpeg4 -r 20 %s_curvature.mp4\" % (min(nproc,16), outpath, datapath.split('/')[-2].split('.')[0]))\n",
    "        print(command, flush=True)\n",
    "        os.system(command)\n",
    "        if os.path.isfile('curvature.mp4'):\n",
    "            os.system('rm -r ' + outpath)\n",
    "        else:\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        print('Error while rendering movie:\\n%s\\n -- please try to manually convert the .png files generated in %s.' % (e, outpath), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processing_type == 'curvature' and not in_script:\n",
    "    curvature_frame (0, verbose=False, save=False, recalculate=False, tarpath=None, force_recalc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processing_type == 'curvature':\n",
    "    print(\"CURVATURE PROCESSING DONE.\", flush=True)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
